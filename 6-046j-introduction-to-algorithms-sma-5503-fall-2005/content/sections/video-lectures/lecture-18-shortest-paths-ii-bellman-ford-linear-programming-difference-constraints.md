---
about_this_resource_text: '<p><strong>Topics covered:</strong>&nbsp;Shortest Paths
  II: Bellman-Ford, Linear Programming, Difference Constraints</p><p><strong>Instructors:</strong>&nbsp;Prof.
  Erik Demaine,&nbsp;Prof. Charles Leiserson</p>'
course_id: 6-046j-introduction-to-algorithms-sma-5503-fall-2005
embedded_media:
- id: lec18.pdf
  parent_uid: e942b39b3005c3e60aeafd8ef255efda
  technical_location: https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-introduction-to-algorithms-sma-5503-fall-2005/video-lectures/lecture-18-shortest-paths-ii-bellman-ford-linear-programming-difference-constraints/lec18.pdf
  title: lec18.pdf
  type: null
  uid: 512c0e7956cecb119d5478dc30255901
- id: 6_046J_lec18_th.jpg
  parent_uid: e942b39b3005c3e60aeafd8ef255efda
  technical_location: https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-introduction-to-algorithms-sma-5503-fall-2005/video-lectures/lecture-18-shortest-paths-ii-bellman-ford-linear-programming-difference-constraints/6_046J_lec18_th.jpg
  title: 6_046J_lec18_th.jpg
  type: null
  uid: cb6c520a6a19bbac6eb81b8aa58ef93c
- id: Video-YouTube-Stream
  media_location: Ttezuzs39nk
  parent_uid: e942b39b3005c3e60aeafd8ef255efda
  title: Video-YouTube-Stream
  type: Video
  uid: d645f4725152203df4b1599ce9c73527
- id: Thumbnail-YouTube-JPG
  media_location: https://img.youtube.com/vi/Ttezuzs39nk/default.jpg
  parent_uid: e942b39b3005c3e60aeafd8ef255efda
  title: Thumbnail-YouTube-JPG
  type: Thumbnail
  uid: 9158ff0f2fcba535ab75e9f3e3360910
- id: Video-InternetArchive-MP4
  media_location: http://www.archive.org/download/MIT6.046JF05MPEG4/ocw-6.046-16nov2005-220k.mp4
  parent_uid: e942b39b3005c3e60aeafd8ef255efda
  title: Video-Internet Archive-MP4
  type: Video
  uid: 8b97b4d58c8cfd90a750950861b9806b
- id: Video-InternetArchive-MP3
  media_location: http://ia310826.us.archive.org/1/items/MIT6.046JF05/ocw-6.046-16nov2005.mp3
  parent_uid: e942b39b3005c3e60aeafd8ef255efda
  title: Video-Internet Archive-MP3
  type: Video
  uid: d119b0181f6a0c9bd58f83cf79a7eae1
- id: Video-VideoLecturesnet-Stream
  media_location: http://videolectures.net/mit6046jf05_introduction_algorithms/
  parent_uid: e942b39b3005c3e60aeafd8ef255efda
  title: Video-VideoLectures.net-Stream
  type: Video
  uid: 4c8adad82fe714bde9f53087fad4786a
- id: Video-iTunesU-MP4
  media_location: http://itunes.apple.com/gb/podcast/lecture-18-shortest-paths/id341597754?i=63738846
  parent_uid: e942b39b3005c3e60aeafd8ef255efda
  title: Video-iTunes U-MP4
  type: Video
  uid: 2eb05ab4acf4d312e093627edf756b33
- id: Video-iTunesU-MP3
  media_location: http://itunes.apple.com/gb/podcast/lecture-18-shortest-paths/id341597751?i=63738818
  parent_uid: e942b39b3005c3e60aeafd8ef255efda
  title: Video-iTunes U-MP3
  type: Video
  uid: 58a4c32dc87616b1758403025cff15d2
- id: Thumbnail-OCW-JPG
  parent_uid: e942b39b3005c3e60aeafd8ef255efda
  title: Thumbnail-OCW-JPG
  type: Thumbnail
  uid: 92e27cb8049aa003d9b2c2a1d698f752
- id: 3Play-3PlayYouTubeid-MP4
  media_location: Ttezuzs39nk
  parent_uid: e942b39b3005c3e60aeafd8ef255efda
  title: 3Play-3Play YouTube id
  type: 3Play
  uid: be190ce621ed056a87f2b44a6096e874
- id: Ttezuzs39nk.srt
  parent_uid: e942b39b3005c3e60aeafd8ef255efda
  technical_location: https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-introduction-to-algorithms-sma-5503-fall-2005/video-lectures/lecture-18-shortest-paths-ii-bellman-ford-linear-programming-difference-constraints/Ttezuzs39nk.srt
  title: 3play caption file
  type: null
  uid: e1d33bce0469110ed7633d808ccdcdde
- id: Ttezuzs39nk.pdf
  parent_uid: e942b39b3005c3e60aeafd8ef255efda
  technical_location: https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-introduction-to-algorithms-sma-5503-fall-2005/video-lectures/lecture-18-shortest-paths-ii-bellman-ford-linear-programming-difference-constraints/Ttezuzs39nk.pdf
  title: 3play pdf file
  type: null
  uid: d4407d384434057fe0733bcb812c336e
- id: Caption-3Play YouTube id-SRT
  parent_uid: e942b39b3005c3e60aeafd8ef255efda
  title: Caption-3Play YouTube id-SRT-English - US
  type: Caption
  uid: c75082771b478d3a439358594879452a
- id: Transcript-3Play YouTube id-PDF
  parent_uid: e942b39b3005c3e60aeafd8ef255efda
  title: Transcript-3Play YouTube id-PDF-English - US
  type: Transcript
  uid: 7b454e461e2604ecaf397c6cdcfe69b1
inline_embed_id: 34432804lecture18:shortestpathsii:bellman-ford,linearprogramming,differenceconstraints26542455
layout: video
order_index: null
parent_uid: c492612542f7cc7a09f73790a5f91d81
related_resources_text: <p>Lecture Notes (<a target="_blank" href="./resolveuid/512c0e7956cecb119d5478dc30255901">PDF</a>)<br
  />             <a target="_blank" href="./resolveuid/efc69ef86c18e164d675bd8808c6477a">Assignments</a><br
  />             <a target="_blank" href="./resolveuid/144d9e513546eac8c1fd9b0d278e6eb2">Exams</a></p>
short_url: lecture-18-shortest-paths-ii-bellman-ford-linear-programming-difference-constraints
technical_location: https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-introduction-to-algorithms-sma-5503-fall-2005/video-lectures/lecture-18-shortest-paths-ii-bellman-ford-linear-programming-difference-constraints
template_type: Tabbed
title: 'Lecture 18: Shortest Paths II: Bellman-Ford, Linear Programming, Difference
  Constraints'
transcript: '<p><span m=''7000''>Good morning, everyone.</span> </p><p><span m=''9000''>Glad
  you are all here bright and early.</span> </p><p><span m=''14000''>I''m counting
  the days till the TA''s outnumber the students.</span> </p><p><span m=''20000''>They''ll
  show up. We return to a familiar story.</span> </p><p><span m=''26000''>This is
  part two, the Empire Strikes Back.</span> </p><p><span m=''32000''>So last time,
  our adversary,</span> <span m=''33000''>the graph, came to us with a problem.</span>
  </p><p><span m=''36000''>We have a source, and we had a directed graph,</span> <span
  m=''39000''>and we had weights on the edges, and they were all</span> <span m=''43000''>nonnegative.
  And there was happiness.</span> </p><p><span m=''46000''>And we triumphed over the
  Empire by designing Dijkstra''s</span> <span m=''50000''>algorithm, and very efficiently
  finding single source shortest</span> <span m=''54000''>paths, shortest path weight
  from s to every other vertex.</span> </p><p><span m=''60000''>Today, however, the
  Death Star has a new trick</span> <span m=''62000''>up its sleeve, and we have negative
  weights,</span> <span m=''65000''>potentially. And we''re going to have to</span>
  <span m=''67000''>somehow deal with, in particular,</span> <span m=''69000''>negative
  weight cycles. And we saw that when we have a</span> <span m=''73000''>negative
  weight cycle, we can just keep going around,</span> <span m=''76000''>and around,
  and around, and go back in time farther,</span> <span m=''79000''>and farther, and
  farther.</span> </p><p><span m=''81000''>And we can get to be arbitrarily far back
  in the</span> <span m=''84000''>past. And so there''s no shortest</span> <span m=''86000''>path,
  because whatever path you take you can get a shorter one.</span> </p><p><span m=''89000''>So
  we want to address that issue today, and we''re going to</span> <span m=''93000''>come
  up with a new algorithm actually simpler than Dijkstra,</span> <span m=''97000''>but
  not as fast, called the Bellman-Ford</span> <span m=''99000''>algorithm. And, it''s
  going to allow</span> <span m=''104000''>negative weights, and in some sense allow</span>
  <span m=''108000''>negative weight cycles, although maybe not as much as</span>
  <span m=''114000''>you might hope. We have to leave room for a</span> <span m=''119000''>sequel,
  of course. OK, so the Bellman-Ford</span> <span m=''124000''>algorithm, invented
  by two guys, as you might expect,</span> <span m=''129000''>it computes the shortest
  path weights.</span> </p><p><span m=''133000''>So, it makes no assumption about
  the weights.</span> </p><p><span m=''137000''>Weights are arbitrary, and it''s going
  to compute the</span> <span m=''142000''>shortest path weights. So, remember this
  notation:</span> <span m=''147000''>delta of s, v is the weight of the shortest
  path from s to v.</span> </p><p><span m=''153000''>s was called a source vertex.
  And, we want to compute these</span> <span m=''160000''>weights for all vertices,
  little v.</span> </p><p><span m=''163000''>The claim is that computing from s to
  everywhere is no</span> <span m=''167000''>harder than computing s to a particular
  location.</span> </p><p><span m=''171000''>So, we''re going to do for all them.</span>
  </p><p><span m=''173000''>It''s still going to be the case here.</span> </p><p><span
  m=''176000''>And, it allows negative weights.</span> </p><p><span m=''179000''>And
  this is the good case, but there''s an alternative,</span> <span m=''183000''>which
  is that Bellman-Ford may just say, oops,</span> <span m=''187000''>there''s a negative
  weight cycle.</span> </p><p><span m=''191000''>And in that case it will just say
  so.</span> </p><p><span m=''194000''>So, they say a negative weight cycle exists.</span>
  </p><p><span m=''198000''>Therefore, some of these deltas are minus infinity.</span>
  </p><p><span m=''203000''>And that seems weird. So, Bellman-Ford as we''ll</span>
  <span m=''207000''>present it today is intended for the case, but there are no</span>
  <span m=''213000''>negative weights cycles, which is more intuitive.</span> </p><p><span
  m=''219000''>It sort of allows them, but it will just report them.</span> </p><p><span
  m=''222000''>In that case, it will not give you delta</span> <span m=''225000''>values.
  You can change the algorithm to</span> <span m=''228000''>give you delta values
  in that case, but we are not going to</span> <span m=''232000''>see it here. So,
  in exercise,</span> <span m=''234000''>after you see the algorithm, exercise is:</span>
  <span m=''237000''>compute these deltas in all cases.</span> </p><p><span m=''252000''>So,
  it''s not hard to do. But we don''t have time for it</span> <span m=''259000''>here.
  So, here''s the algorithm.</span> </p><p><span m=''264000''>It''s pretty straightforward.
  As I said, it''s easier than</span> <span m=''272000''>Dijkstra. It''s a relaxation
  algorithm.</span> </p><p><span m=''276000''>So the main thing that it does is relax
  edges just like</span> <span m=''280000''>Dijkstra. So, we''ll be able to use a
  lot</span> <span m=''283000''>of dilemmas from Dijkstra. And proof of correctness
  will</span> <span m=''287000''>be three times shorter because the first two thirds
  we already</span> <span m=''291000''>have from Dijkstra. But I''m jumping ahead
  a bit.</span> </p><p><span m=''295000''>So, the first part is initialization.</span>
  </p><p><span m=''297000''>Again, d of v will represent the estimated distance from
  s to</span> <span m=''301000''>v. And we''re going to be updating</span> <span m=''305000''>those
  estimates as the algorithm goes along.</span> </p><p><span m=''308000''>And initially,
  d of s is zero,</span> <span m=''310000''>which now may not be the right answer
  conceivably.</span> </p><p><span m=''314000''>Everyone else is infinity, which is
  certainly an upper</span> <span m=''317000''>bound. OK, these are both upper bounds</span>
  <span m=''320000''>on the true distance. So that''s fine.</span> </p><p><span m=''323000''>That''s
  initialization just like before.</span> </p><p><span m=''336000''>And now we have
  a main loop which happens v minus one times.</span> </p><p><span m=''339000''>We''re
  not actually going to use the index i.</span> </p><p><span m=''341000''>It''s just
  a counter.</span> </p><p><span m=''362000''>And we''re just going to look at every
  edge and relax it.</span> </p><p><span m=''367000''>It''s a very simple idea. If
  you learn about relaxation,</span> <span m=''373000''>this is the first thing you
  might try.</span> </p><p><span m=''376000''>The question is when do you stop.</span>
  </p><p><span m=''380000''>It''s sort of like I have this friend to what he was like
  six</span> <span m=''385000''>years old he would claim, oh, I know how to spell
  banana.</span> </p><p><span m=''391000''>I just don''t know when to stop. OK, same
  thing with relaxation.</span> </p><p><span m=''397000''>This is our relaxation step
  just as before.</span> </p><p><span m=''400000''>We look at the edge; we see whether
  it violates the</span> <span m=''403000''>triangle inequality according to our current
  estimates we know</span> <span m=''407000''>the distance from s to v should be at
  most distance from s to</span> <span m=''411000''>plus the weight of that edge from
  u to v.</span> </p><p><span m=''414000''>If it isn''t, we set it equal.</span> </p><p><span
  m=''415000''>We''ve proved that this is always an OK thing to do.</span> </p><p><span
  m=''420000''>We never violate, I mean, these d of v''s never</span> <span m=''423000''>get
  too small if we do a bunch of relaxations.</span> </p><p><span m=''427000''>So,
  the idea is you take every edge.</span> </p><p><span m=''429000''>You relax it.
  I don''t care which order.</span> </p><p><span m=''432000''>Just relax every edge,
  one each.</span> </p><p><span m=''435000''>And that do that V minus one times.</span>
  </p><p><span m=''437000''>The claim is that that should be enough if you have no</span>
  <span m=''441000''>negative weights cycles. So, if there''s a negative</span> <span
  m=''445000''>weight cycle, we need to figure it out.</span> </p><p><span m=''450000''>And,
  we''ll do that in a fairly straightforward way,</span> <span m=''455000''>which
  is we''re going to do exactly the same thing.</span> </p><p><span m=''460000''>So
  this is outside before loop here.</span> </p><p><span m=''464000''>We''ll have the
  same four loops for each edge in our graph.</span> </p><p><span m=''470000''>We''ll
  try to relax it. And if you can relax it,</span> <span m=''474000''>the claim is
  that there has to be a negative weight cycle.</span> </p><p><span m=''482000''>So
  this is the main thing that needs proof.</span> </p><p><span m=''508000''>OK, and
  that''s the algorithm. So the claim is that at the</span> <span m=''511000''>ends
  we should have d of v, let''s see, L''s so to speak.</span> </p><p><span m=''515000''>d
  of v equals delta of s comma v for every vertex,</span> <span m=''518000''>v. If
  we don''t find a negative</span> <span m=''520000''>weight cycle according to this
  rule, that we should have all</span> <span m=''524000''>the shortest path weights.
  That''s the claim.</span> </p><p><span m=''527000''>Now, the first question is,
  in here, the running time is</span> <span m=''530000''>very easy to analyze. So
  let''s start with the running</span> <span m=''534000''>time. We can compare it
  to Dijkstra,</span> <span m=''536000''>which is over here. What is the running time
  of</span> <span m=''542000''>this algorithm? V times E, exactly.</span> </p><p><span
  m=''546000''>OK, I''m going to assume, because it''s pretty reasonable,</span> <span
  m=''552000''>that V and E are both positive. Then it''s V times E.</span> </p><p><span
  m=''559000''>So, this is a little bit slower, or a fair amount slower,</span> <span
  m=''565000''>than Dijkstra''s algorithm. There it is:</span> <span m=''570000''>E
  plus V log V is essentially, ignoring the logs is pretty</span> <span m=''575000''>much
  linear time. Here we have something that''s</span> <span m=''579000''>at least quadratic
  in V, assuming your graph is</span> <span m=''583000''>connected. So, it''s slower,</span>
  <span m=''585000''>but it''s going to handle these negative weights.</span> </p><p><span
  m=''588000''>Dijkstra can''t handle negative weights at all.</span> </p><p><span
  m=''592000''>So, let''s do an example, make it clear why you might</span> <span
  m=''596000''>hope this algorithm works. And then we''ll prove that it</span> <span
  m=''603000''>works, of course. But the proof will be pretty</span> <span m=''608000''>easy.
  So, I''m going to draw a graph</span> <span m=''612000''>that has negative weights,
  but no negative weight cycles</span> <span m=''618000''>so that I get an interesting
  answer.</span> </p><p><span m=''655000''>Good. The other thing I need in order</span>
  <span m=''657000''>to make the output of this algorithm well defined,</span> <span
  m=''660000''>it depends in which order you visit the edges.</span> </p><p><span
  m=''663000''>So I''m going to assign an arbitrary order to these edges.</span> </p><p><span
  m=''667000''>I could just ask you for an order, but to be consistent with</span>
  <span m=''671000''>the notes, I''ll put an ordering on it.</span> </p><p><span m=''673000''>Let''s
  say I put number four, say that''s the fourth edge I''ll</span> <span m=''677000''>visit.
  It doesn''t matter.</span> </p><p><span m=''678000''>But it will affect what happens
  during the algorithm for a</span> <span m=''682000''>particular graph.</span> </p><p><span
  m=''703000''>Do they get them all? One, two, three,</span> <span m=''706000''>four,
  five, six, seven, eight,</span> <span m=''708000''>OK. And my source is going to
  be A.</span> </p><p><span m=''711000''>And, that''s it. So, I want to run this</span>
  <span m=''714000''>algorithm. I''m just going to initialize</span> <span m=''717000''>everything.
  So, I set the estimates for s</span> <span m=''721000''>to be zero, and everyone
  else to be infinity.</span> </p><p><span m=''726000''>And to give me some notion
  of time, over here I''m going to</span> <span m=''730000''>draw or write down what
  all of these d values are as the</span> <span m=''735000''>algorithm proceeds because
  I''m going to start crossing them out</span> <span m=''740000''>and rewriting them
  that the figure will get a little bit</span> <span m=''745000''>messier. But we
  can keep track of it</span> <span m=''748000''>over here. It''s initially zero and</span>
  <span m=''751000''>infinities. Yeah?</span> </p><p><span m=''754000''>It doesn''t
  matter. So, for the algorithm you can</span> <span m=''756000''>go to the edges
  in a different order every time if you want.</span> </p><p><span m=''760000''>We''ll
  prove that, but here I''m going to go</span> <span m=''762000''>through the same
  order every time.</span> </p><p><span m=''764000''>Good question. It turns out it
  doesn''t matter</span> <span m=''767000''>here. OK, so here''s the starting</span>
  <span m=''769000''>point. Now I''m going to relax every</span> <span m=''771000''>edge.
  So, there''s going to be a lot</span> <span m=''773000''>of edges here that don''t
  do anything.</span> </p><p><span m=''775000''>I try to relax n minus one. I''d say,
  well,</span> <span m=''777000''>I know how to get from s to B with weight infinity.</span>
  </p><p><span m=''782000''>Infinity plus two I can get to from s to E.</span> </p><p><span
  m=''784000''>Well, infinity plus two is not much better than infinity.</span> </p><p><span
  m=''788000''>OK, so I don''t do anything, don''t update this to infinity.</span>
  </p><p><span m=''791000''>I mean, infinity plus two sounds even worse.</span> </p><p><span
  m=''794000''>But infinity plus two is infinity.</span> </p><p><span m=''796000''>OK,
  that''s the edge number one. So, no relaxation edge number</span> <span m=''800000''>two,
  same deal as number three, same deal, edge number four we</span> <span m=''804000''>start
  to get something interesting because I have a</span> <span m=''807000''>finite value
  here that says I can get from A to B using a</span> <span m=''811000''>total weight
  of minus one. So that seems good.</span> </p><p><span m=''815000''>I''ll write down
  minus one here, and update B to minus one.</span> </p><p><span m=''821000''>The
  rest stay the same. So, I''m just going to keep</span> <span m=''825000''>doing
  this over and over. That was edge number four.</span> </p><p><span m=''830000''>Number
  five, we also get a relaxation.</span> </p><p><span m=''833000''>Four is better
  than infinity. So, c gets a number of four.</span> </p><p><span m=''840000''>Then
  we get to edge number six. That''s infinity plus five is</span> <span m=''844000''>worse
  than four. OK, so no relaxation there.</span> </p><p><span m=''847000''>Edge number
  seven is interesting because I have a</span> <span m=''851000''>finite value here
  minus one plus the weight of this edge,</span> <span m=''855000''>which is three.
  That''s a total of two,</span> <span m=''858000''>which is actually better than
  four.</span> </p><p><span m=''860000''>So, this route, A, B, c is actually better
  than</span> <span m=''864000''>the route I just found a second ago.</span> </p><p><span
  m=''866000''>So, this is now a two. This is all happening in one</span> <span m=''870000''>iteration
  of the main loop. We actually found two good</span> <span m=''875000''>paths to
  c. We found one better than the</span> <span m=''878000''>other. OK, and that was
  edge number</span> <span m=''881000''>seven, and edge number eight is over here.</span>
  </p><p><span m=''884000''>It doesn''t matter. OK, so that was round one of</span>
  <span m=''887000''>this outer loop, so, the first value of i.</span> </p><p><span
  m=''890000''>i equals one. OK, now we continue.</span> </p><p><span m=''892000''>Just
  keep going. So, we start with edge number</span> <span m=''896000''>one. Now, minus
  one plus two is one.</span> </p><p><span m=''900000''>That''s better than infinity.
  It''ll start speeding up.</span> </p><p><span m=''904000''>It''s repetitive. It''s
  actually not too much</span> <span m=''908000''>longer until we''re done. Number
  two, this is an infinity</span> <span m=''914000''>so we don''t do anything. Number
  three:</span> <span m=''917000''>minus one plus two is one; better than infinity.</span>
  </p><p><span m=''922000''>This is vertex d, and it''s number three.</span> </p><p><span
  m=''925000''>Number four we''ve already done. Nothing changed.</span> </p><p><span
  m=''931000''>Number five: this is where we see the path</span> <span m=''935000''>four
  again, but that''s worse than two.</span> </p><p><span m=''938000''>So, we don''t
  update anything. Number six: one plus five is</span> <span m=''943000''>six, which
  is bigger than two, so no good.</span> </p><p><span m=''947000''>Go around this
  way. Number seven:</span> <span m=''949000''>same deal. Number eight is interesting.</span>
  </p><p><span m=''953000''>So, we have a weight of one here, a weight of minus three</span>
  <span m=''958000''>here. So, the total is minus two,</span> <span m=''962000''>which
  is better than one. So, that was d.</span> </p><p><span m=''967000''>And, I believe
  that''s it. So that was definitely the end</span> <span m=''973000''>of that round.
  So, it''s I plus two because we</span> <span m=''978000''>just looked at the eighth
  edge. And, I''ll cheat and check.</span> </p><p><span m=''984000''>Indeed, that
  is the last thing that happens.</span> </p><p><span m=''990000''>We can check the
  couple of outgoing edges from d because</span> <span m=''993000''>that''s the only
  one whose value just changed.</span> </p><p><span m=''996000''>And, there are no
  more relaxations possible.</span> </p><p><span m=''999000''>So, that was in two
  rounds. The claim is we got all the</span> <span m=''1003000''>shortest path weights.
  The algorithm would actually</span> <span m=''1007000''>loop four times to guarantee
  correctness because we have five</span> <span m=''1011000''>vertices here and one
  less than that.</span> </p><p><span m=''1013000''>So, in fact, in the execution
  here there are</span> <span m=''1016000''>two more blank rounds at the bottom.</span>
  </p><p><span m=''1019000''>Nothing happens. But, what the hell?</span> </p><p><span
  m=''1023000''>OK, so that is Bellman-Ford. I mean, it''s certainly not</span> <span
  m=''1026000''>doing anything wrong. The question is,</span> <span m=''1028000''>why
  is it guaranteed to converge in V minus one steps</span> <span m=''1031000''>unless
  there is a negative weight cycle?</span> </p><p><span m=''1033000''>Question?</span>
  </p><p><span m=''1044000''>Right, so that''s an optimization.</span> </p><p><span
  m=''1045000''>If you discover a whole round, and nothing happens,</span> <span m=''1048000''>so
  you can keep track of that in the algorithm thing,</span> <span m=''1051000''>you
  can stop. In the worst case,</span> <span m=''1053000''>it won''t make a difference.
  But in practice,</span> <span m=''1055000''>you probably want to do that. Yeah?</span>
  </p><p><span m=''1057000''>Good question. All right, so some simple</span> <span
  m=''1060000''>observations, I mean, we''re only doing</span> <span m=''1062000''>relaxation.
  So, we can use a lot of our</span> <span m=''1064000''>analysis from before. In
  particular,</span> <span m=''1066000''>the d values are only decreasing monotonically.</span>
  </p><p><span m=''1069000''>As we cross out values here, we are always making it</span>
  <span m=''1071000''>smaller, which is good. Another nifty thing about this</span>
  <span m=''1074000''>algorithm is that you can run it even in a distributed system.</span>
  </p><p><span m=''1080000''>If this is some actual network, some computer network,</span>
  <span m=''1082000''>and these are machines, and they''re communicating by</span>
  <span m=''1085000''>these links, I mean, it''s a purely local thing.</span> </p><p><span
  m=''1087000''>Relaxation is a local thing. You don''t need any global</span> <span
  m=''1089000''>strategy, and you''re asking about, can we do a different</span> <span
  m=''1092000''>order in each step? Well, yeah, you could just keep</span> <span m=''1095000''>relaxing
  edges, and keep relaxing edges,</span> <span m=''1096000''>and just keep going for
  the entire lifetime of the network.</span> </p><p><span m=''1099000''>And eventually,
  you will find shortest paths.</span> </p><p><span m=''1101000''>So, this algorithm
  is guaranteed to finish in V rounds</span> <span m=''1104000''>in a distributed
  system. It might be more asynchronous.</span> </p><p><span m=''1107000''>And, it''s
  a little harder to analyze.</span> </p><p><span m=''1110000''>But it will still
  work eventually.</span> </p><p><span m=''1114000''>It''s guaranteed to converge.
  And so, Bellman-Ford is used in</span> <span m=''1121000''>the Internet for finding
  shortest paths.</span> </p><p><span m=''1126000''>OK, so let''s finally prove that
  it works.</span> </p><p><span m=''1131000''>This should only take a couple of boards.</span>
  </p><p><span m=''1136000''>So let''s suppose we have a graph and some edge weights
  that</span> <span m=''1143000''>have no negative weight cycles. Then the claim is
  that we</span> <span m=''1153000''>terminate with the correct answer.</span> </p><p><span
  m=''1159000''>So, Bellman-Ford terminates with all of these d of v values</span>
  <span m=''1169000''>set to the delta values for every vertex.</span> </p><p><span
  m=''1178000''>OK, the proof is going to be pretty immediate using the</span> <span
  m=''1182000''>lemmas that we had from before if you remember them.</span> </p><p><span
  m=''1185000''>So, we''re just going to look at every vertex separately.</span> </p><p><span
  m=''1190000''>So, I''ll call the vertex v. The claim is that this holds by</span>
  <span m=''1194000''>the end of the algorithm. So, remember what we need to</span>
  <span m=''1198000''>prove is that at some point, d of v equals delta of s comma</span>
  <span m=''1202000''>v because we know it decreases monotonically,</span> <span m=''1206000''>and
  we know that it never gets any smaller than the correct</span> <span m=''1210000''>value
  because relaxations are always safe.</span> </p><p><span m=''1215000''>So, we just
  need to show at some point this holds,</span> <span m=''1224000''>and that it will
  hold at the end.</span> </p><p><span m=''1232000''>So, by monotonicity of the d
  values, and by correctness part</span> <span m=''1241000''>one, which was that the
  d of v''s are always greater than or equal</span> <span m=''1251000''>to the deltas,
  we only need to show that at</span> <span m=''1258000''>some point we have equality.</span>
  </p><p><span m=''1278000''>So that''s our goal. So what we''re going to do is</span>
  <span m=''1281000''>just look at v, and the shortest path to v,</span> <span m=''1284000''>and
  see what happens to the algorithm relative to that path.</span> </p><p><span m=''1290000''>So,
  I''m going to name the path. Let''s call it p.</span> </p><p><span m=''1295000''>It
  starts at vertex v_0 and goes to v_1, v_2,</span> <span m=''1300000''>whatever,
  and ends at v_k. And, this is not just any</span> <span m=''1306000''>shortest path,
  but it''s one that starts at s.</span> </p><p><span m=''1311000''>So, v_0''s s,
  and it ends at v.</span> </p><p><span m=''1314000''>So, I''m going to give a couple
  of names to s and v so I can</span> <span m=''1321000''>talk about the path more
  uniformly.</span> </p><p><span m=''1324000''>So, this is a shortest path from s
  to v.</span> </p><p><span m=''1331000''>Now, I also want it to be not just any shortest
  path from s to</span> <span m=''1335000''>v, but among all shortest paths from s
  to v I want it to be one</span> <span m=''1340000''>with the fewest possible edges.</span>
  </p><p><span m=''1352000''>OK, so shortest here means in terms of the total weight
  of the</span> <span m=''1356000''>path. Subject to being shortest in</span> <span
  m=''1358000''>weight, I wanted to also be shortest in the number of edges.</span>
  </p><p><span m=''1362000''>And, the reason I want that is to be able to conclude
  that p is</span> <span m=''1366000''>a simple path, meaning that it doesn''t repeat</span>
  <span m=''1370000''>any vertices. Now, can anyone tell me why I</span> <span m=''1372000''>need
  to assume that the number of edges is the smallest</span> <span m=''1376000''>possible
  in order to guarantee that p is simple?</span> </p><p><span m=''1381000''>The claim
  is that not all shortest paths are necessarily</span> <span m=''1384000''>simple.
  Yeah?</span> </p><p><span m=''1385000''>Right, I can have a zero weight cycle, exactly.</span>
  </p><p><span m=''1387000''>So, we are hoping, I mean, in fact in the theorem</span>
  <span m=''1390000''>here, we''re assuming that there are no negative weight cycles.</span>
  </p><p><span m=''1394000''>But there might be zero weight cycles still.</span> </p><p><span
  m=''1397000''>As a zero weight cycle, you can put that in the middle</span> <span
  m=''1400000''>of any shortest path to make it arbitrarily long,</span> <span m=''1403000''>repeat
  vertices over and over. That''s going to be annoying.</span> </p><p><span m=''1406000''>What
  I want is that p is simple.</span> </p><p><span m=''1410000''>And, I can guarantee
  that essentially by shortcutting.</span> </p><p><span m=''1413000''>If ever I take
  a zero weight cycle, I throw it away.</span> </p><p><span m=''1416000''>And this
  is one mathematical way of doing that.</span> </p><p><span m=''1419000''>OK, now
  what else do we know about this shortest path?</span> </p><p><span m=''1423000''>Well,
  we know that subpaths are shortest paths are shortest</span> <span m=''1427000''>paths.
  That''s optimal substructure.</span> </p><p><span m=''1429000''>So, we know what
  the shortest path from s to v_i is sort of</span> <span m=''1433000''>inductively.
  It''s the shortest path,</span> <span m=''1435000''>I mean, it''s the weight of
  that path, which is,</span> <span m=''1438000''>in particular, the shortest path
  from s to v</span> <span m=''1441000''>minus one plus the weight of the last edge,
  v minus one to v_i.</span> </p><p><span m=''1447000''>So, this is by optimal substructure
  as we proved last</span> <span m=''1457000''>time. OK, and I think that''s pretty</span>
  <span m=''1463000''>much the warm-up. So, I want to sort of do this</span> <span
  m=''1470000''>inductively in I, start out with v zero,</span> <span m=''1473000''>and
  go up to v_k. So, the first question is,</span> <span m=''1477000''>what is d of
  v_0, which is s?</span> </p><p><span m=''1480000''>What is d of the source? Well,
  certainly at the</span> <span m=''1484000''>beginning of the algorithm, it''s zero.</span>
  </p><p><span m=''1487000''>So, let''s say equals zero initially because that''s
  what we</span> <span m=''1492000''>set it to. And it only goes down from</span>
  <span m=''1495000''>there. So, it certainly,</span> <span m=''1497000''>at most,
  zero. The real question is,</span> <span m=''1501000''>what is delta of s comma
  v_0. What is the shortest path</span> <span m=''1506000''>weight from s to s? It
  has to be zero,</span> <span m=''1509000''>otherwise you have a negative weight
  cycle,</span> <span m=''1513000''>exactly. My favorite answer,</span> <span m=''1515000''>zero.
  So, if we had another path from</span> <span m=''1519000''>s to s, I mean, that
  is a cycle.</span> </p><p><span m=''1521000''>So, it''s got to be zero. So, these
  are actually equal at</span> <span m=''1526000''>the beginning of the algorithm,
  which is great.</span> </p><p><span m=''1532000''>That means they will be for all
  time because we just argued up</span> <span m=''1537000''>here, only goes down,
  never can get too small.</span> </p><p><span m=''1541000''>So, we have d of v_0
  set to the right thing.</span> </p><p><span m=''1545000''>Great: good for the base
  case of the induction.</span> </p><p><span m=''1549000''>Of course, what we really
  care about is v_k,</span> <span m=''1553000''>which is v. So, let''s talk about
  the v_i</span> <span m=''1556000''>inductively, and then we will get v_k as a result.</span>
  </p><p><span m=''1571000''>So, yeah, let''s do it by induction.</span> </p><p><span
  m=''1574000''>That''s more fun.</span> </p><p><span m=''1587000''>Let''s say that
  d of v_i is equal to delta of s v_i after I</span> <span m=''1592000''>rounds of
  the algorithm. So, this is actually referring</span> <span m=''1598000''>to the
  I that is in the algorithm here.</span> </p><p><span m=''1602000''>These are rounds.
  So, one round is an entire</span> <span m=''1606000''>execution of all the edges,
  relaxation of all the edges.</span> </p><p><span m=''1612000''>So, this is certainly
  true for I equals zero.</span> </p><p><span m=''1616000''>We just proved that. After
  zero rounds,</span> <span m=''1620000''>at the beginning of the algorithm, d of
  v_0 equals delta</span> <span m=''1626000''>of s, v_0. OK, so now, that''s not really</span>
  <span m=''1631000''>what I wanted, but OK, fine.</span> </p><p><span m=''1633000''>Now
  we''ll prove it for d of v_i plus one.</span> </p><p><span m=''1636000''>Generally,
  I recommend you assume something.</span> </p><p><span m=''1640000''>In fact, why
  don''t I follow my own advice and change it?</span> </p><p><span m=''1644000''>It''s
  usually nicer to think of induction as recursion.</span> </p><p><span m=''1649000''>So,
  you assume that this is true, let''s say,</span> <span m=''1652000''>for j less
  than the i that you care about, and then you prove</span> <span m=''1657000''>it
  for d of v_i. It''s usually a lot easier to</span> <span m=''1662000''>think about
  it that way. In particular,</span> <span m=''1664000''>you can use strong induction
  for all less than i.</span> </p><p><span m=''1668000''>Here, we''re only going to
  need it for one less.</span> </p><p><span m=''1671000''>We have some relation between
  I and I minus one here in terms of</span> <span m=''1676000''>the deltas. And so,
  we want to argue</span> <span m=''1679000''>something about the d values. OK, well,
  let''s think about</span> <span m=''1685000''>what''s going on here. We know that,</span>
  <span m=''1688000''>let''s say, after I minus one rounds, we have this inductive</span>
  <span m=''1695000''>hypothesis, d of v_i minus one equals delta of s v_i minus one.</span>
  </p><p><span m=''1702000''>And, we want to conclude that after i rounds,</span>
  <span m=''1707000''>so we have one more round to do this.</span> </p><p><span m=''1711000''>We
  want to conclude that d of v_i has the right answer,</span> <span m=''1718000''>delta
  of s comma v_i. Does that look familiar at all?</span> </p><p><span m=''1724000''>So
  we want to relax every edge in this round.</span> </p><p><span m=''1727000''>In
  particular, at some point,</span> <span m=''1729000''>we have to relax the edge
  from v_i minus one to v_i.</span> </p><p><span m=''1733000''>We know that this path
  consists of edges.</span> </p><p><span m=''1736000''>That''s the definition of a
  path.</span> </p><p><span m=''1740000''>So, during the i''th round, we relax every
  edge.</span> </p><p><span m=''1750000''>So, we better relax v_i minus one v_i.</span>
  </p><p><span m=''1758000''>And, what happens then? It''s a test of memory.</span>
  </p><p><span m=''1783000''>Quick, the Death Star is approaching.</span> </p><p><span
  m=''1786000''>So, if we have the correct value for v_i minus one,</span> <span m=''1791000''>that
  we relax an outgoing edge from there, and that edge is an</span> <span m=''1797000''>edge
  of the shortest path from s to v_i.</span> </p><p><span m=''1801000''>What do we
  know? d of v_i becomes the correct</span> <span m=''1807000''>value, delta of s
  comma v_i. This was called correctness</span> <span m=''1813000''>lemma last time.
  One of the things we proved</span> <span m=''1818000''>about Dijkstra''s algorithm,
  but it was really just a fact</span> <span m=''1824000''>about relaxation. And it
  was a pretty simple</span> <span m=''1829000''>proof. And it comes from this fact.</span>
  </p><p><span m=''1832000''>We know the shortest path weight is this.</span> </p><p><span
  m=''1835000''>So, certainly d of v_i was at least this big,</span> <span m=''1838000''>and
  let''s suppose it''s greater, or otherwise we were done.</span> </p><p><span m=''1842000''>We
  know d of v_i minus one is set to this.</span> </p><p><span m=''1844000''>And so,
  this is exactly the condition that''s being checked</span> <span m=''1848000''>in
  the relaxation step. And, the d of v_i value will be</span> <span m=''1852000''>greater
  than this, let''s suppose.</span> </p><p><span m=''1854000''>And then, we''ll set
  it equal to this.</span> </p><p><span m=''1856000''>And that''s exactly d of s v_i.
  So, when we relax that edge,</span> <span m=''1861000''>we''ve got to set it to
  the right value.</span> </p><p><span m=''1864000''>So, this is the end of the proof,
  right?</span> </p><p><span m=''1866000''>It''s very simple. The point is,</span>
  <span m=''1868000''>you look at your shortest path. Here it is.</span> </p><p><span
  m=''1871000''>And if we assume there''s no negative weight cycles,</span> <span
  m=''1874000''>this has the correct value initially.</span> </p><p><span m=''1877000''>d
  of s is going to be zero. After the first round,</span> <span m=''1880000''>you''ve
  got to relax this edge. And then you get the right</span> <span m=''1883000''>value
  for that vertex. After the second round,</span> <span m=''1886000''>you''ve got
  to relax this edge, which gets you the right d</span> <span m=''1890000''>value
  for this vertex and so on. And so, no matter which</span> <span m=''1896000''>shortest
  path you take, you can apply this analysis.</span> </p><p><span m=''1900000''>And
  you know that by, if the length of this path,</span> <span m=''1904000''>here we
  assumed it was k edges, then after k rounds you''ve got</span> <span m=''1910000''>to
  be done. OK, so this was not actually</span> <span m=''1913000''>the end of the
  proof. Sorry.</span> </p><p><span m=''1917000''>So this means after k rounds, we
  have the right answer for</span> <span m=''1923000''>v_k, which is v. So, the only
  question is how</span> <span m=''1928000''>big could k be? And, it better be the
  right</span> <span m=''1932000''>answer, at most, v minus one is the claim by the</span>
  <span m=''1938000''>algorithm that you only need to do v minus one steps.</span>
  </p><p><span m=''1944000''>And indeed, the number of edges in a simple path in a
  graph is,</span> <span m=''1950000''>at most, the number of vertices minus one.</span>
  </p><p><span m=''1957000''>k is, at most, v minus one because p is</span> <span
  m=''1960000''>simple. So, that''s why we had to assume</span> <span m=''1963000''>that
  it wasn''t just any shortest path.</span> </p><p><span m=''1967000''>It had to be
  a simple one so it didn''t repeat any vertices.</span> </p><p><span m=''1972000''>So
  there are, at most, V vertices in the</span> <span m=''1975000''>path, so at most,
  V minus one edges in the path.</span> </p><p><span m=''1981000''>OK, and that''s
  all there is to Bellman-Ford.</span> </p><p><span m=''1985000''>So: pretty simple
  in correctness.</span> </p><p><span m=''1988000''>Of course, we''re using a lot
  of the lemmas that we proved last</span> <span m=''1995000''>time, which makes it
  easier. OK, a consequence of this</span> <span m=''2001000''>theorem, or of this
  proof is that if Bellman-Ford fails to</span> <span m=''2007000''>converge, and
  that''s what the algorithm is checking is whether</span> <span m=''2013000''>this
  relaxation still requires work after these d minus one</span> <span m=''2019000''>steps.
  Right, the end of this</span> <span m=''2024000''>algorithm is run another round,
  a V''th round,</span> <span m=''2028000''>see whether anything changes. So, we''ll
  say that the</span> <span m=''2033000''>algorithm fails to converge after V minus
  one steps or</span> <span m=''2038000''>rounds. Then, there has to be a</span> <span
  m=''2041000''>negative weight cycle. OK, this is just a</span> <span m=''2044000''>contrapositive
  of what we proved.</span> </p><p><span m=''2046000''>We proved that if you assume
  there''s no negative weight</span> <span m=''2050000''>cycle, then we know that
  d of s is zero, and then all this</span> <span m=''2054000''>argument says is you''ve
  got to converge after v minus one</span> <span m=''2058000''>rounds. There can''t
  be anything left to</span> <span m=''2061000''>do once you''ve reached the shortest
  path weights because</span> <span m=''2064000''>you''re going monotonically; you
  can never hit the bottom.</span> </p><p><span m=''2070000''>You can never go to
  the floor. So, if you fail to converge</span> <span m=''2073000''>somehow after
  V minus one rounds, you''ve got to have</span> <span m=''2077000''>violated the
  assumption. The only assumption we made was</span> <span m=''2080000''>there''s
  no negative weight cycle.</span> </p><p><span m=''2082000''>So, this tells us that
  Bellman-Ford is actually</span> <span m=''2085000''>correct. When it says that there
  is a</span> <span m=''2088000''>negative weight cycle, it indeed means it.</span>
  </p><p><span m=''2091000''>It''s true. OK, and you can modify</span> <span m=''2093000''>Bellman-Ford
  in that case to sort of run a little longer,</span> <span m=''2096000''>and find
  where all the minus infinities are.</span> </p><p><span m=''2101000''>And that is,
  in some sense,</span> <span m=''2102000''>one of the things you have to do in your
  problem set,</span> <span m=''2105000''>I believe. So, I won''t cover it here.</span>
  </p><p><span m=''2108000''>But, it''s a good exercise in any case to figure out
  how you</span> <span m=''2111000''>would find where the minus infinities are.</span>
  </p><p><span m=''2114000''>What are all the vertices reachable from negative weight</span>
  <span m=''2118000''>cycle? Those are the ones that have</span> <span m=''2120000''>minus
  infinities. OK, so you might say,</span> <span m=''2122000''>well, that was awfully
  fast. Actually, it''s not over yet.</span> </p><p><span m=''2126000''>The episode
  is not yet ended. We''re going to use Bellman-Ford</span> <span m=''2129000''>to
  solve the even bigger and greater shortest path problems.</span> </p><p><span m=''2135000''>And
  in the remainder of today''s lecture, we will see it applied</span> <span m=''2139000''>to
  a more general problem, in some sense,</span> <span m=''2142000''>called linear
  programming. And the next lecture,</span> <span m=''2145000''>we''ll really use
  it to do some amazing stuff with all pairs</span> <span m=''2149000''>shortest paths.
  Let''s go over here.</span> </p><p><span m=''2152000''>So, our goal, although it
  won''t be obvious</span> <span m=''2155000''>today, is to be able to compute the
  shortest paths between every</span> <span m=''2159000''>pair of vertices, which
  we could certainly do at</span> <span m=''2163000''>this point just by running Bellman-Ford
  v times.</span> </p><p><span m=''2168000''>OK, but we want to do better than that,
  of course.</span> </p><p><span m=''2175000''>And, that will be the climax of the
  trilogy.</span> </p><p><span m=''2181000''>OK, today we just discovered who Luke''s
  father is.</span> </p><p><span m=''2190000''>So, it turns out the father of shortest
  paths is linear</span> <span m=''2197000''>programming. Actually, simultaneously
  the</span> <span m=''2202000''>father and the mother because programs do not have
  gender.</span> </p><p><span m=''2210000''>OK, my father likes to say, we both took
  improv comedy</span> <span m=''2217000''>lessons so we have degrees in improvisation.</span>
  </p><p><span m=''2225000''>And he said, you know, we went to improv</span> <span
  m=''2227000''>classes in order to learn how to make our humor better.</span> </p><p><span
  m=''2230000''>And, the problem is, it didn''t actually make our</span> <span m=''2233000''>humor
  better. It just made us less afraid to</span> <span m=''2236000''>use it. [LAUGHTER]
  So,</span> <span m=''2237000''>you are subjected to all this improv humor.</span>
  </p><p><span m=''2240000''>I didn''t see the connection of Luke''s father,</span>
  <span m=''2242000''>but there you go. OK, so, linear programming is a</span> <span
  m=''2245000''>very general problem, a very big tool.</span> </p><p><span m=''2249000''>Has
  anyone seen linear programming before?</span> </p><p><span m=''2252000''>OK, one
  person. And, I''m sure you will,</span> <span m=''2256000''>at some time in your
  life, do anything vaguely computing</span> <span m=''2260000''>optimization related,
  linear programming comes up at</span> <span m=''2265000''>some point. It''s a very
  useful tool.</span> </p><p><span m=''2268000''>You''re given a matrix and two vectors:
  not too exciting yet.</span> </p><p><span m=''2273000''>What you want to do is find
  a vector.</span> </p><p><span m=''2277000''>This is a very dry description. We''ll
  see what makes it so</span> <span m=''2282000''>interesting in a moment.</span>
  </p><p><span m=''2297000''>So, you want to maximize some objective, and you have
  some</span> <span m=''2301000''>constraints. And they''re all linear.</span> </p><p><span
  m=''2304000''>So, the objective is a linear function in the variables x,</span>
  <span m=''2308000''>and your constraints are a bunch of linear constraints,</span>
  <span m=''2312000''>inequality constraints, that''s one makes an</span> <span m=''2316000''>interesting.
  It''s not just solving a linear</span> <span m=''2319000''>system as you''ve seen
  in linear algebra, or whatever.</span> </p><p><span m=''2323000''>Or, of course,
  it could be that there is no</span> <span m=''2326000''>such x. OK: vaguely familiar
  you might</span> <span m=''2329000''>think to the theorem about Bellman-Ford.</span>
  </p><p><span m=''2332000''>And, we''ll show that there''s some kind of connection
  here</span> <span m=''2336000''>that either you want to find something, or show
  that it</span> <span m=''2341000''>doesn''t exist. Well, that''s still a pretty</span>
  <span m=''2346000''>vague connection, but I also want to maximize</span> <span m=''2349000''>something,
  or are sort of minimize the shortest paths,</span> <span m=''2353000''>OK, somewhat
  similar. We have these constraints.</span> </p><p><span m=''2357000''>So, yeah.
  This may be intuitive to you,</span> <span m=''2359000''>I don''t know. I prefer
  a more geometric</span> <span m=''2362000''>picture, and I will try to draw such
  a geometric picture,</span> <span m=''2367000''>and I''ve never tried to do this
  on a blackboard,</span> <span m=''2370000''>so it should be interesting. I think
  I''m going to fail</span> <span m=''2376000''>miserably. It sort of looks like a</span>
  <span m=''2379000''>dodecahedron, right?</span> </p><p><span m=''2381000''>Sort
  of, kind of, not really.</span> </p><p><span m=''2384000''>A bit rough on the bottom,
  OK.</span> </p><p><span m=''2387000''>So, if you have a bunch of linear constraints,</span>
  <span m=''2391000''>this is supposed to be in 3-D. Now I labeled it.</span> </p><p><span
  m=''2396000''>It''s now in 3-D. Good.</span> </p><p><span m=''2400000''>So, you
  have these linear constraints.</span> </p><p><span m=''2402000''>That turns out
  to define hyperplanes in n dimensions.</span> </p><p><span m=''2406000''>OK, so
  you have this base here that''s three-dimensional space.</span> </p><p><span m=''2411000''>So,
  n equals three. And, these hyperplanes,</span> <span m=''2414000''>if you''re looking
  at one side of the hyperplane,</span> <span m=''2417000''>that''s the less than
  or equal to, if you take the</span> <span m=''2421000''>intersection, you get some
  convex polytope or</span> <span m=''2424000''>polyhedron. In 3-D, you might get
  a</span> <span m=''2427000''>dodecahedron or whatever. And, your goal,</span> <span
  m=''2429000''>you have some objective vector c, let''s say,</span> <span m=''2433000''>up.
  Suppose that''s the c vector.</span> </p><p><span m=''2437000''>Your goal is to
  find the highest point in this polytope.</span> </p><p><span m=''2442000''>So here,
  it''s maybe this one. OK, this is the target.</span> </p><p><span m=''2447000''>This
  is the optimal, x.</span> </p><p><span m=''2449000''>That is the geometric view.
  If you prefer the algebraic</span> <span m=''2454000''>view, you want to maximize
  the c transpose times x.</span> </p><p><span m=''2460000''>So, this is m. This is
  n.</span> </p><p><span m=''2461000''>Check out the dimensions work out.</span> </p><p><span
  m=''2464000''>So that''s saying you want to maximize the dot product.</span> </p><p><span
  m=''2468000''>You want to maximize the extent to which x is in the direction</span>
  <span m=''2473000''>c. And, you want to maximize that</span> <span m=''2476000''>subject
  to some constraints, which looks something like</span> <span m=''2480000''>this,
  maybe. So, this is A,</span> <span m=''2482000''>and it''s m by n. You want to multiply
  it by,</span> <span m=''2485000''>it should be something of height n.</span> </p><p><span
  m=''2490000''>That''s x. Let me put x down here,</span> <span m=''2492000''>n by
  one. And, it should be less than or</span> <span m=''2496000''>equal to something
  of this height, which is B,</span> <span m=''2499000''>the right hand side. OK,
  that''s the algebraic view,</span> <span m=''2504000''>which is to check out all
  the dimensions are working out.</span> </p><p><span m=''2508000''>But, you can read
  these off in each row here,</span> <span m=''2512000''>when multiplied by this column,
  gives you one value here.</span> </p><p><span m=''2517000''>And as just a linear
  constraints on all the x sides.</span> </p><p><span m=''2523000''>So, you want to
  maximize this linear function of x_1 up to x_n</span> <span m=''2528000''>subject
  to these constraints, OK?</span> </p><p><span m=''2531000''>Pretty simple, but pretty
  powerful in general.</span> </p><p><span m=''2536000''>So, it turns out that with,
  you can formulate a huge number</span> <span m=''2541000''>of problems such as shortest
  paths as a linear program.</span> </p><p><span m=''2546000''>So, it''s a general
  tool. And in this class,</span> <span m=''2551000''>we will not cover any algorithms
  for solving linear</span> <span m=''2557000''>programming. It''s a bit tricky.</span>
  </p><p><span m=''2560000''>I''ll just mention that they are out there.</span> </p><p><span
  m=''2564000''>So, there''s many efficient algorithms, and lots of code</span> <span
  m=''2570000''>that does this. It''s a very practical setup.</span> </p><p><span
  m=''2575000''>So, lots of algorithms to solve LP''s, linear programs.</span> </p><p><span
  m=''2582000''>Linear programming is usually called LP.</span> </p><p><span m=''2585000''>And,
  I''ll mention a few of them.</span> </p><p><span m=''2588000''>There''s the simplex
  algorithm. This is one of the first.</span> </p><p><span m=''2594000''>I think it
  is the first, the ellipsoid algorithm.</span> </p><p><span m=''2598000''>There''s
  interior point methods, and there''s random sampling.</span> </p><p><span m=''2604000''>I''ll
  just say a little bit about each of these because</span> <span m=''2609000''>we''re
  not going to talk about any of them in depth.</span> </p><p><span m=''2616000''>The
  simplex algorithm, this is, I mean,</span> <span m=''2618000''>one of the first
  algorithms in the world in some sense,</span> <span m=''2621000''>certainly one
  of the most popular.</span> </p><p><span m=''2623000''>It''s still used today. Almost
  all linear programming</span> <span m=''2627000''>code uses the simplex algorithm.
  It happens to run an</span> <span m=''2630000''>exponential time in the worst-case,
  so it''s actually</span> <span m=''2633000''>pretty bad theoretically. But in practice,</span>
  <span m=''2636000''>it works really well. And there is some recent work</span> <span
  m=''2639000''>that tries to understand this. It''s still exponential in the</span>
  <span m=''2643000''>worst case. But, it''s practical.</span> </p><p><span m=''2646000''>There''s
  actually an open problem whether there exists a</span> <span m=''2650000''>variation
  of simplex that runs in polynomial time.</span> </p><p><span m=''2653000''>But,
  I won''t go into that. That''s a major open problem in</span> <span m=''2657000''>this
  area of linear programming. The ellipsoid algorithm was the</span> <span m=''2662000''>first
  algorithm to solve linear programming in polynomial time.</span> </p><p><span m=''2666000''>So,
  for a long time, people didn''t know.</span> </p><p><span m=''2670000''>Around this
  time, people started realizing</span> <span m=''2672000''>polynomial time is a good
  thing. That happened around the late</span> <span m=''2676000''>60s. Polynomial
  time is good.</span> </p><p><span m=''2677000''>And, the ellipsoid algorithm is
  the first one to do it.</span> </p><p><span m=''2681000''>It''s a very general algorithm,
  and very powerful,</span> <span m=''2684000''>theoretically: completely impractical.</span>
  </p><p><span m=''2686000''>But, it''s cool. It lets you do things like you</span>
  <span m=''2689000''>can solve a linear program that has exponentially many</span>
  <span m=''2692000''>constraints in polynomial time. You''ve got all sorts of crazy</span>
  <span m=''2696000''>things. So, I''ll just say it''s</span> <span m=''2697000''>polynomial
  time. I can''t say something nice</span> <span m=''2701000''>about it; don''t say
  it at all. It''s impractical.</span> </p><p><span m=''2704000''>Interior point methods
  are sort of the mixture.</span> </p><p><span m=''2707000''>They run in polynomial
  time. You can guarantee that.</span> </p><p><span m=''2711000''>And, they are also
  pretty practical, and there''s sort of</span> <span m=''2714000''>this competition
  these days about whether simplex or</span> <span m=''2718000''>interior point is
  better. And, I don''t know what it is</span> <span m=''2721000''>today but a few
  years ago they were neck and neck.</span> </p><p><span m=''2724000''>And, random
  sampling is a brand new approach.</span> </p><p><span m=''2727000''>This is just
  from a couple years ago by two MIT professors,</span> <span m=''2731000''>Dimitris
  Bertsimas and Santosh Vempala, I guess the other is in</span> <span m=''2735000''>applied
  math. So, just to show you,</span> <span m=''2739000''>there''s active work in this
  area.</span> </p><p><span m=''2741000''>People are still finding new ways to solve
  linear programs.</span> </p><p><span m=''2744000''>This is completely randomized,
  and very simple,</span> <span m=''2747000''>and very general. It hasn''t been implemented,</span>
  <span m=''2750000''>so we don''t know how practical it is yet.</span> </p><p><span
  m=''2752000''>But, it has potential. OK: pretty neat.</span> </p><p><span m=''2754000''>OK,
  we''re going to look at a somewhat simpler version of</span> <span m=''2757000''>linear
  programming. The first restriction we are</span> <span m=''2762000''>going to make
  is actually not much of a restriction.</span> </p><p><span m=''2765000''>But, nonetheless
  we will consider it, it''s a little bit</span> <span m=''2769000''>easier to think
  about. So here, we had some polytope</span> <span m=''2773000''>we wanted to maximize
  some objective.</span> </p><p><span m=''2776000''>In a feasibility problem, I just
  want to know,</span> <span m=''2779000''>is the polytope empty? Can you find any
  point in that</span> <span m=''2783000''>polytope? Can you find any set of values,</span>
  <span m=''2786000''>x, that satisfy these constraints?</span> </p><p><span m=''2790000''>OK,
  so there''s no objective. c, just find x such that AX is</span> <span m=''2794000''>less
  than or equal to B. OK, it turns out you can prove</span> <span m=''2799000''>a
  very general theorem that if you can solve linear</span> <span m=''2803000''>feasibility,
  you can also solve linear programming.</span> </p><p><span m=''2807000''>We won''t
  prove that here, but this is actually no easier</span> <span m=''2812000''>than
  the original problem even though it feels easier,</span> <span m=''2816000''>and
  it''s easier to think about. I was just saying actually no</span> <span m=''2823000''>easier
  than LP. OK, the next restriction we''re</span> <span m=''2828000''>going to make
  is a real restriction.</span> </p><p><span m=''2831000''>And it simplifies the problem
  quite a bit.</span> </p><p><span m=''2850000''>And that''s to look at different
  constraints.</span> </p><p><span m=''2855000''>And, if all this seemed a bit abstract
  so far,</span> <span m=''2860000''>we will now ground ourselves little bit.</span>
  </p><p><span m=''2865000''>A system of different constraints is a linear</span>
  <span m=''2871000''>feasibility problem. So, it''s an LP where there''s no</span>
  <span m=''2877000''>objective. And, it''s with a restriction,</span> <span m=''2886000''>so,
  where each row of the matrix, so, the matrix,</span> <span m=''2897000''>A, has
  one one, and it has one minus one,</span> <span m=''2906000''>and everything else
  in the row is zero.</span> </p><p><span m=''2916000''>OK, in other words, each constraint
  has its very</span> <span m=''2920000''>simple form. It involves two variables and</span>
  <span m=''2925000''>some number. So, we have something like x_j</span> <span m=''2929000''>minus
  x_i is less than or equal to w_ij.</span> </p><p><span m=''2933000''>So, this is
  just a number. These are two variables.</span> </p><p><span m=''2940000''>There''s
  a minus sign, no values up here,</span> <span m=''2942000''>no coefficients, no
  other of the X_k''s appear,</span> <span m=''2946000''>just two of them. And, you
  have a bunch of</span> <span m=''2949000''>constraints of this form, one per row
  of the matrix.</span> </p><p><span m=''2953000''>Geometrically, I haven''t thought
  about what</span> <span m=''2956000''>this means. I think it means the</span> <span
  m=''2958000''>hyperplanes are pretty simple. Sorry I can''t do better than</span>
  <span m=''2962000''>that. It''s a little hard to see this</span> <span m=''2965000''>in
  high dimensions. But, it will start to</span> <span m=''2970000''>correspond to
  something we''ve seen, namely the board that its</span> <span m=''2978000''>next
  to, very shortly. OK, so let''s do a very quick</span> <span m=''2985000''>example
  mainly to have something to point at.</span> </p><p><span m=''2990000''>Here''s
  a very simple system of difference constraints --</span> <span m=''3011000''>--
  OK, and a solution. Why not?</span> </p><p><span m=''3013000''>It''s not totally
  trivial to solve this, but here''s a</span> <span m=''3018000''>solution. And the
  only thing to check is</span> <span m=''3021000''>that each of these constraints
  is satisfied.</span> </p><p><span m=''3025000''>x_1 minus x_2 is three, which is
  less than or equal to</span> <span m=''3029000''>three, and so on. There could be
  negative values.</span> </p><p><span m=''3035000''>There could be positive values.
  It doesn''t matter.</span> </p><p><span m=''3042000''>I''d like to transform this
  system of difference constraints</span> <span m=''3049000''>into a graph because
  we know a lot about graphs.</span> </p><p><span m=''3055000''>So, we''re going to
  call this the constraint graph.</span> </p><p><span m=''3063000''>And, it''s going
  to represent these constraints.</span> </p><p><span m=''3068000''>How''d I do it?
  Well, I take every constraint,</span> <span m=''3073000''>which in general looks
  like this, and I convert it into an</span> <span m=''3080000''>edge. OK, so if I
  write it as x_j</span> <span m=''3084000''>minus x_i is less than or equal to some
  w_ij,</span> <span m=''3089000''>w seems suggestive of weights. That''s exactly
  why I called it</span> <span m=''3096000''>w. I''m going to make that an edge</span>
  <span m=''3098000''>from v_i to v_j. So, the order flips a little</span> <span m=''3101000''>bit.
  And, the weight of that edge is</span> <span m=''3104000''>w_ij. So, just do that.</span>
  </p><p><span m=''3106000''>Make n vertices. So, you have the number of</span> <span
  m=''3109000''>vertices equals n. The number of edges equals the</span> <span m=''3113000''>number
  of constraints, which is m, the height of the</span> <span m=''3116000''>matrix,
  and just transform. So, for example,</span> <span m=''3121000''>here we have three
  variables. So, we have three vertices,</span> <span m=''3126000''>v_1, v_2, v_3.
  We have x_1 minus x_2.</span> </p><p><span m=''3129000''>So, we have an edge from
  v_2 to v_1 of weight three.</span> </p><p><span m=''3134000''>We have x_2 minus
  x_3. So, we have an edge from v_3 to</span> <span m=''3138000''>v_2 of weight minus
  two. And, we have x_1 minus x_3.</span> </p><p><span m=''3143000''>So, we have an
  edge from v_3 to v_1 of weight two.</span> </p><p><span m=''3147000''>I hope I got
  the directions right.</span> </p><p><span m=''3152000''>Yep. So, there it is,</span>
  <span m=''3154000''>a graph: currently no obvious connection to shortest paths,</span>
  <span m=''3160000''>right? But in fact,</span> <span m=''3162000''>this constraint
  is closely related to shortest paths.</span> </p><p><span m=''3167000''>So let me
  just rewrite it. You could say,</span> <span m=''3172000''>well, an x_j is less
  than or equal to x_i plus w_ij.</span> </p><p><span m=''3179000''>Or, you could
  think of it as d[j] less than or equal to d[i]</span> <span m=''3183000''>plus w_ij.
  This is a conceptual balloon.</span> </p><p><span m=''3187000''>Look awfully familiar?
  A lot like the triangle</span> <span m=''3190000''>inequality, a lot like relaxation.</span>
  </p><p><span m=''3193000''>So, there''s a very close connection between these two</span>
  <span m=''3197000''>problems as we will now prove.</span> </p><p><span m=''3223000''>So,
  we''re going to have two theorems.</span> </p><p><span m=''3225000''>And, they''re
  going to look similar to the correctness of</span> <span m=''3229000''>Bellman-Ford
  in that they talk about negative weight cycles.</span> </p><p><span m=''3233000''>Here
  we go. It turns out,</span> <span m=''3234000''>I mean, we have this constraint
  graph.</span> </p><p><span m=''3237000''>It can have negative weights. It can have
  positive weights.</span> </p><p><span m=''3242000''>It turns out what matters is
  if you have a negative weight</span> <span m=''3245000''>cycle. So, the first thing
  to prove is</span> <span m=''3247000''>that if you have a negative weight cycle
  that something bad</span> <span m=''3251000''>happens. OK, what could happen bad?</span>
  </p><p><span m=''3253000''>Well, we''re just trying to satisfy this system of</span>
  <span m=''3256000''>constraints. So, the bad thing is that there</span> <span m=''3259000''>might
  not be any solution. These constraints may be</span> <span m=''3262000''>infeasible.
  And that''s the claim.</span> </p><p><span m=''3264000''>The claim is that this
  is actually an if and only if.</span> </p><p><span m=''3269000''>But first we''ll
  proved the if. If you have a negative weight</span> <span m=''3273000''>cycle, you''re
  doomed. The difference constraints are</span> <span m=''3278000''>unsatisfiable.
  That''s a more intuitive way to</span> <span m=''3281000''>say it. In the LP world,</span>
  <span m=''3283000''>they call it infeasible. But unsatisfiable makes a lot</span>
  <span m=''3288000''>more sense. There''s no way to assign the</span> <span m=''3291000''>x_i''s
  in order to satisfy all the constraints simultaneously.</span> </p><p><span m=''3296000''>So,
  let''s just take a look. Consider a negative weight</span> <span m=''3301000''>cycle.
  It starts at some vertex,</span> <span m=''3303000''>goes through some vertices,
  and at some point comes back.</span> </p><p><span m=''3307000''>I don''t care whether
  it repeats vertices, just as long as this</span> <span m=''3311000''>cycle, from
  v_1 to v_1 is a negative weight cycle strictly</span> <span m=''3315000''>negative
  weight.</span> </p><p><span m=''3326000''>OK, and what I''m going to do is just
  write down all the</span> <span m=''3330000''>constraints. Each of these edges corresponds</span>
  <span m=''3334000''>to a constraint, which must be in the set of</span> <span m=''3337000''>constraints
  because we had that graph.</span> </p><p><span m=''3340000''>So, these are all edges.
  Let''s look at what they give</span> <span m=''3345000''>us. So, we have an edge
  from v_1 to</span> <span m=''3348000''>v_2. That corresponds to x_2 minus</span>
  <span m=''3350000''>x_1 is, at most, something, w_12.</span> </p><p><span m=''3353000''>Then
  we have x_3 minus x_2. That''s the weight w_23,</span> <span m=''3357000''>and so
  on. And eventually we get up to</span> <span m=''3364000''>something like x_k minus
  x_(k-1).</span> </p><p><span m=''3368000''>That''s this edge: w_(k-1),k , and lastly
  we have</span> <span m=''3375000''>this edge, which wraps around. So, it''s x_1
  minus x_k,</span> <span m=''3383000''>w_k1 if I''ve got the signs right.</span>
  </p><p><span m=''3390000''>Good, so here''s a bunch of constraints.</span> </p><p><span
  m=''3395000''>What do you suggest I do with them?</span> </p><p><span m=''3400000''>Anything
  interesting about these constraints,</span> <span m=''3407000''>say, the left hand
  sides? Sorry?</span> </p><p><span m=''3412000''>It sounded like the right word.
  What was it?</span> </p><p><span m=''3420000''>Telescopes, yes, good.</span> </p><p><span
  m=''3421000''>Everything cancels. If I added these up,</span> <span m=''3424000''>there''s
  an x_2 and a minus x_2. There''s a minus x_1 and an x_1.</span> </p><p><span m=''3428000''>There''s
  a minus XK and an XK. Everything here cancels if I</span> <span m=''3432000''>add
  up the left hand sides. So, what happens if I add up</span> <span m=''3435000''>the
  right hand sides? Over here I get zero,</span> <span m=''3438000''>my favorite answer.
  And over here,</span> <span m=''3440000''>we get all the weights of all the edges
  in the negative weight</span> <span m=''3444000''>cycle, which is the weight of
  the cycle, which is negative.</span> </p><p><span m=''3450000''>So, zero is strictly
  less than zero: contradiction.</span> </p><p><span m=''3453000''>Contradiction:
  wait a minute,</span> <span m=''3455000''>we didn''t assume anything that was false.</span>
  </p><p><span m=''3457000''>So, it''s not really a contradiction in the</span> <span
  m=''3460000''>mathematical sense. We didn''t contradict the world.</span> </p><p><span
  m=''3463000''>We just said that these constraints are contradictory.</span> </p><p><span
  m=''3467000''>In other words, if you pick any values of the</span> <span m=''3470000''>x_i''s,
  there is no way that these can all be true because</span> <span m=''3473000''>that
  you would get a contradiction.</span> </p><p><span m=''3475000''>So, it''s impossible
  for these things to be satisfied by some</span> <span m=''3479000''>real x_i''s.
  So, these must be</span> <span m=''3481000''>unsatisfiable. Let''s say there''s
  no satisfying</span> <span m=''3487000''>assignment, a little more precise, x_1
  up to x_m,</span> <span m=''3491000''>no weights. Can we satisfy those</span> <span
  m=''3494000''>constraints? Because they add up to zero on</span> <span m=''3498000''>the
  left-hand side, and negative on the right-hand</span> <span m=''3503000''>side.
  OK, so that''s an easy proof.</span> </p><p><span m=''3506000''>The reverse direction
  will be only slightly harder.</span> </p><p><span m=''3513000''>OK, so, cool. We
  have this connection.</span> </p><p><span m=''3514000''>So motivation is, suppose
  you''d want to solve</span> <span m=''3517000''>these difference constraints. And
  we''ll see one such</span> <span m=''3520000''>application. I Googled around for
  difference</span> <span m=''3522000''>constraints. There is a fair number of</span>
  <span m=''3524000''>papers that care about difference constraints.</span> </p><p><span
  m=''3526000''>And, they all use shortest paths to solve them.</span> </p><p><span
  m=''3529000''>So, if we can prove a connection between shortest</span> <span m=''3531000''>paths,
  which we know how to compute, and difference</span> <span m=''3534000''>constraints,
  then we''ll have something cool.</span> </p><p><span m=''3536000''>And, next class
  will see even more applications of difference</span> <span m=''3540000''>constraints.
  It turns out they''re really</span> <span m=''3545000''>useful for all pairs shortest
  paths.</span> </p><p><span m=''3549000''>OK, but for now let''s just prove this
  equivalence and</span> <span m=''3556000''>finish it off. So, the reverse direction
  is if</span> <span m=''3561000''>there''s no negative weight cycle in this constraint
  graph,</span> <span m=''3569000''>then the system better be satisfiable.</span>
  </p><p><span m=''3575000''>The claim is that these negative weight cycles are the</span>
  <span m=''3582000''>only barriers for finding a solution to these difference</span>
  <span m=''3589000''>constraints. I have this feeling somewhere</span> <span m=''3594000''>here.
  I had to talk about the</span> <span m=''3598000''>constraint graph. Good.</span>
  </p><p><span m=''3613000''>Satisfied, good. So, here we''re going to see a</span>
  <span m=''3619830''>technique that is very useful when thinking about shortest</span>
  <span m=''3628482''>paths. And, it''s a bit hard to guess,</span> <span m=''3632788''>especially
  if you haven''t seen it before.</span> </p><p><span m=''3636505''>This is useful
  in problem sets, and in quizzes,</span> <span m=''3640780''>and finals, and everything.
  So, keep this in mind.</span> </p><p><span m=''3645334''>I mean, I''m using it to
  prove this rather simple theorem,</span> <span m=''3650539''>but the idea of changing
  the graph, so I''m going to call this</span> <span m=''3656115''>constraint graph
  G. Changing the graph is a very</span> <span m=''3660483''>powerful idea. So, we''re
  going to add a new</span> <span m=''3664386''>vertex, s, or source, use the source,</span>
  <span m=''3667732''>Luke, and we''re going to add a bunch of edges from s because</span>
  <span m=''3673215''>being a source, it better be connected to some</span> <span
  m=''3677397''>things. So, we are going to add a zero</span> <span m=''3683529''>weight
  edge, or weight zero edge from s to everywhere,</span> <span m=''3689764''>so, to
  every other vertex in the constraint graph.</span> </p><p><span m=''3696000''>Those
  vertices are called v_i, v_1 up to v_n.</span> </p><p><span m=''3700121''>So, I
  have my constraint graph. But I''ll copy this one so I can</span> <span m=''3705928''>change
  it. It''s always good to backup your</span> <span m=''3709768''>work before you
  make changes, right?</span> </p><p><span m=''3713046''>So now, I want to add a new
  vertex, s, over here,</span> <span m=''3717542''>my new source. I just take my constraint</span>
  <span m=''3721195''>graph, whatever it looks like, add in weight zero edges to all</span>
  <span m=''3726909''>the other vertices. Simple enough.</span> </p><p><span m=''3731171''>Now,
  what did I do? What did you do?</span> </p><p><span m=''3734100''>Well, I have a
  candidate source now which can reach all the</span> <span m=''3738953''>vertices.
  So, shortest path from s,</span> <span m=''3741799''>hopefully, well, paths from
  s exist.</span> </p><p><span m=''3744728''>I can get from s to everywhere in weight
  at most zero.</span> </p><p><span m=''3750000''>OK, maybe less. Could it be less?</span>
  </p><p><span m=''3751851''>Well, you know, like v_2, I can get to it by</span> <span
  m=''3754338''>zero minus two. So, that''s less than zero.</span> </p><p><span m=''3756710''>So
  I''ve got to be a little careful.</span> </p><p><span m=''3758677''>What if there''s
  a negative weight cycle?</span> </p><p><span m=''3760933''>Oh no? Then there wouldn''t
  be any</span> <span m=''3762785''>shortest paths. Fortunately,</span> <span m=''3764347''>we
  assume that there''s no negative weight cycle in the</span> <span m=''3767413''>original
  graph. And if you think about it,</span> <span m=''3769785''>if there''s no negative
  weight cycle in the original graph,</span> <span m=''3773082''>we add an edge from
  s to everywhere else.</span> </p><p><span m=''3775396''>We''re not making any new
  negative weight cycles because</span> <span m=''3778520''>you can start at s and
  go somewhere at a cost of zero,</span> <span m=''3781586''>which doesn''t affect
  any weights.</span> </p><p><span m=''3785000''>And then, you are forced to stay
  in the old graph.</span> </p><p><span m=''3788920''>So, there can''t be any new
  negative weight cycles.</span> </p><p><span m=''3792840''>So, the modified graph
  has no negative weight cycles.</span> </p><p><span m=''3797000''>That''s good because
  it also has paths from s,</span> <span m=''3800519''>and therefore it also has shortest
  paths from s.</span> </p><p><span m=''3805000''>The modified graph has no negative
  weight because it</span> <span m=''3810376''>didn''t before. And, it has paths from
  s.</span> </p><p><span m=''3814487''>There''s a path from s to every vertex.</span>
  </p><p><span m=''3818387''>There may not have been before. Before, I couldn''t get
  from v_2</span> <span m=''3824923''>to v_3, for example. Well, that''s still true.</span>
  </p><p><span m=''3829561''>But from s I can get to everywhere.</span> </p><p><span
  m=''3833145''>So, that means that this graph, this modified graph,</span> <span
  m=''3838521''>has shortest paths. Shortest paths exist from s.</span> </p><p><span
  m=''3844974''>In other words, if I took all the shortest path</span> <span m=''3849860''>weights,
  like I ran Bellman-Ford from s, then,</span> <span m=''3854641''>I would get a bunch
  of finite numbers, d of v,</span> <span m=''3859421''>for every value, for every
  vertex.</span> </p><p><span m=''3862926''>That seems like a good idea. Let''s do
  it.</span> </p><p><span m=''3867175''>So, shortest paths exist. Let''s just assign
  x_i to be the</span> <span m=''3873757''>shortest path weight from s to v_i.</span>
  </p><p><span m=''3876782''>Why not? That''s a good choice for a</span> <span m=''3879806''>number,
  the shortest path weight from s to v_i.</span> </p><p><span m=''3883898''>This is
  finite because it''s less than infinity,</span> <span m=''3887990''>and it''s greater
  than minus infinity, so,</span> <span m=''3891549''>some finite number. That''s
  what we need to do in</span> <span m=''3895730''>order to satisfy these constraints.</span>
  </p><p><span m=''3900000''>The claim is that this is a satisfying assignment.</span>
  </p><p><span m=''3903933''>Why? Triangle inequality.</span> </p><p><span m=''3905860''>Somewhere
  here we wrote triangle inequality.</span> </p><p><span m=''3909311''>This looks
  a lot like the triangle inequality.</span> </p><p><span m=''3912924''>In fact, I
  think that''s the end of the proof.</span> </p><p><span m=''3916456''>Let''s see
  here. What we want to be true with</span> <span m=''3919908''>this assignment is
  that x_j minus x_i is less than or equal</span> <span m=''3924564''>to w_ij whenever
  ij is an edge. Or, let''s say v_i,</span> <span m=''3928497''>v_j, for every such
  constraint, so, for v_i,</span> <span m=''3931949''>v_j in the edge set. OK, so
  what is this true?</span> </p><p><span m=''3937313''>Well, let''s just expand it
  out. So, x_i is this delta,</span> <span m=''3942217''>and x_j is some other delta.
  So, we have delta of s,</span> <span m=''3946935''>vj minus delta of s_vi. And,
  on the right-hand side,</span> <span m=''3951654''>well, w_ij, that was the weight
  of the edge from I to J.</span> </p><p><span m=''3956743''>So, this is the weight
  of v_i to v_j.</span> </p><p><span m=''3961000''>OK, I will rewrite this slightly.</span>
  </p><p><span m=''3963659''>Delta s, vj is less than or equal to delta s,</span>
  <span m=''3967315''>vi plus w of v_i, v_j.</span> </p><p><span m=''3969060''>And
  that''s the triangle inequality more or less.</span> </p><p><span m=''3972965''>The
  shortest path from s to v_j is, at most, shortest path from</span> <span m=''3978117''>s
  to v_i plus a particular path from v_i to v_j,</span> <span m=''3982022''>namely
  the single edge v_i to v_j.</span> </p><p><span m=''3984765''>This could only be
  longer than the shortest path.</span> </p><p><span m=''3990000''>And so, that makes
  the right-hand side bigger,</span> <span m=''3993372''>which makes this inequality
  more true, meaning it was true</span> <span m=''3997644''>before. And now it''s
  still true.</span> </p><p><span m=''3999967''>And, that proves it. This is true.</span>
  </p><p><span m=''4002441''>And, these were all equivalent statements.</span> </p><p><span
  m=''4005513''>This we know to be true by triangle inequality.</span> </p><p><span
  m=''4008961''>Therefore, these constraints are all satisfied.</span> </p><p><span
  m=''4012408''>Magic. I''m so excited here.</span> </p><p><span m=''4014357''>So,
  we''ve proved that having a negative weight cycle is exactly</span> <span m=''4019004''>when
  these system of difference constraints are unsatisfiable.</span> </p><p><span m=''4025000''>So,
  if we want to satisfy them, if we want to find the right</span> <span m=''4028241''>answer
  to x, we run Bellman-Ford.</span> </p><p><span m=''4030000''>Either it says, oh,
  no negative weight cycle.</span> </p><p><span m=''4032417''>Then you are hosed.
  Then, there is no solution.</span> </p><p><span m=''4034945''>But that''s the best
  you could hope to know.</span> </p><p><span m=''4037252''>Otherwise, it says, oh,
  there was no negative</span> <span m=''4039670''>weight cycle, and here are your
  shortest path</span> <span m=''4042087''>weights. You just plug them in,</span>
  <span m=''4043736''>and bam, you have your x_i''s that satisfy the constraints.</span>
  </p><p><span m=''4046868''>Awesome. Now, it wasn''t just any graph.</span> </p><p><span
  m=''4050000''>I mean, we started with constraints, algebra,</span> <span m=''4052877''>we
  converted it into a graph by this transform.</span> </p><p><span m=''4055886''>Then
  we added a source vertex, s.</span> </p><p><span m=''4057978''>So, I mean, we had
  to build a graph to solve our problem,</span> <span m=''4061641''>very powerful
  idea. Cool.</span> </p><p><span m=''4063210''>This is the idea of reduction. You
  can reduce the problem you</span> <span m=''4067135''>want to solve into some problem
  you know how to solve.</span> </p><p><span m=''4070601''>You know how to solve shortest
  paths when there are no negative</span> <span m=''4074656''>weight cycles, or find
  out that there is a</span> <span m=''4077337''>negative weight cycle by Bellman-Ford.</span>
  </p><p><span m=''4081000''>So, now we know how to solve difference constraints.</span>
  </p><p><span m=''4086099''>It turns out you can do even more.</span> </p><p><span
  m=''4089400''>Bellman-Ford does a little bit more than just solve these</span> <span
  m=''4095000''>constraints. But first let me write down</span> <span m=''4098899''>what
  I''ve been jumping up and down about.</span> </p><p><span m=''4102899''>The corollary
  is you can use Bellman-Ford.</span> </p><p><span m=''4107000''>I mean, you make
  this graph. Then you apply Bellman-Ford,</span> <span m=''4114484''>and it will
  solve your system of difference constraints.</span> </p><p><span m=''4121330''>So,
  let me put in some numbers here.</span> </p><p><span m=''4125685''>You have m difference
  constraints.</span> </p><p><span m=''4129792''>And, you have n variables. And, it
  will solve them in</span> <span m=''4136265''>order m times n time. Actually, these
  numbers go up</span> <span m=''4142416''>slightly because we are adding n edges,
  and we''re adding one</span> <span m=''4147332''>vertex, but assuming all of these
  numbers are nontrivial,</span> <span m=''4152000''>m is at least n. It''s order
  MN time.</span> </p><p><span m=''4154916''>OK, trying to avoid cases where some
  of them are close to zero.</span> </p><p><span m=''4160082''>Good. So, some other
  facts,</span> <span m=''4162250''>that''s what I just said. And we''ll leave these
  as</span> <span m=''4166250''>exercises because they''re not too essential.</span>
  </p><p><span m=''4171000''>The main thing we need is this. But, some other cool
  facts is</span> <span m=''4175627''>that Bellman-Ford actually optimizes some objective</span>
  <span m=''4179484''>functions. So, we are saying it''s just a</span> <span m=''4182492''>feasibility
  problem. We just want to know whether</span> <span m=''4186193''>these constraints
  are satisfiable.</span> </p><p><span m=''4188738''>In fact, you can add a particular
  objective function.</span> </p><p><span m=''4192750''>So, you can''t give it an
  arbitrary objective function,</span> <span m=''4196837''>but here''s one of interest.
  x_1 plus x_2 plus x_n,</span> <span m=''4204647''>OK, but not just that. We have
  some constraints.</span> </p><p><span m=''4224000''>OK, this is a linear program.
  I want to maximize the sum of</span> <span m=''4227395''>the x_i''s subject to all
  the x_i''s being nonpositive and the</span> <span m=''4230849''>difference constraints.
  So, this we had before.</span> </p><p><span m=''4233542''>This is fine. We noticed
  at some point you</span> <span m=''4235943''>could get from s to everywhere with
  cost, at most,</span> <span m=''4238811''>zero. So, we know that in this</span>
  <span m=''4240509''>assignment all of the x_i''s are negative.</span> </p><p><span
  m=''4242851''>That''s not necessary, but it''s true when you run</span> <span m=''4245602''>Bellman-Ford.
  So if you solve your system</span> <span m=''4247943''>using Bellman-Ford, which
  is no less general than</span> <span m=''4250754''>anything else, you happen to
  get nonpositive</span> <span m=''4253272''>x_i''s. And so, subject to that</span>
  <span m=''4254969''>constraint, it actually makes them is close to zero as</span>
  <span m=''4258072''>possible in the L1 norm. In the sum of these values,</span>
  <span m=''4264009''>it tries to make the sum as close to zero,</span> <span m=''4268577''>it
  tries to make the values as small as possible in absolute</span> <span m=''4275154''>value
  in this sense. OK, it does more than that.</span> </p><p><span m=''4280393''>It
  cooks, it cleans, it finds shortest paths.</span> </p><p><span m=''4285297''>It
  also minimizes the spread, the maximum over all i of x_i</span> <span m=''4291761''>minus
  the minimum over all i of x_i.</span> </p><p><span m=''4297000''>So, I mean, if
  you have your real line, and here are the</span> <span m=''4300840''>x_i''s wherever
  they are. It minimizes this distance.</span> </p><p><span m=''4304402''>And zero
  is somewhere over here.</span> </p><p><span m=''4306567''>So, it tries to make the
  x_i''s as compact as possible.</span> </p><p><span m=''4310268''>This is actually
  the L infinity norm, if you know stuff about</span> <span m=''4314458''>norms from
  your linear algebra class.</span> </p><p><span m=''4316972''>OK, this is the L1
  norm. I think it minimizes every LP</span> <span m=''4320673''>norm. Good, so let''s
  use this for</span> <span m=''4325170''>something. Yeah, let''s solve a real</span>
  <span m=''4329163''>problem, and then we''ll be done for today.</span> </p><p><span
  m=''4333978''>Next class we''ll see the really cool stuff, the really cool</span>
  <span m=''4340790''>application of all of this. For now, and we''ll see a cool</span>
  <span m=''4347366''>but relatively simple application, which is VLSI</span> <span
  m=''4352886''>layout. We talked a little bit about</span> <span m=''4357528''>VLSI
  way back and divide and conquer.</span> </p><p><span m=''4360779''>You have a bunch
  of chips, or you want to arrange them,</span> <span m=''4365655''>and minimize some
  objectives. So, here''s a particular,</span> <span m=''4370441''>tons of problems
  that come out of VLSI layout.</span> </p><p><span m=''4374505''>Here''s one of them.
  You have a bunch of features of</span> <span m=''4379020''>an integrated circuit.
  You want to somehow arrange</span> <span m=''4384583''>them on your circuit without
  putting any two of them too</span> <span m=''4389845''>close to each other. You
  have some minimum</span> <span m=''4393768''>separation like at least they should
  not get top of each</span> <span m=''4399030''>other. Probably, you also need some</span>
  <span m=''4402283''>separation to put wires in between, and so on,</span> <span
  m=''4406589''>so, without putting any two features too close together.</span> </p><p><span
  m=''4413000''>OK, so just to give you an idea, so I have some objects and</span>
  <span m=''4417152''>I''m going to be a little bit vague about how this works.</span>
  </p><p><span m=''4421089''>You have some features. This is stuff,</span> <span m=''4423738''>some
  chips, whatever. We don''t really care what their</span> <span m=''4427460''>shapes
  look like. I just want to be able to move</span> <span m=''4430825''>them around
  so that the gap at any point, so let me just think</span> <span m=''4435192''>about
  this gap. This gap should be at least</span> <span m=''4438199''>some delta. Or,
  I don''t want to use delta.</span> </p><p><span m=''4441134''>Let''s say epsilon,
  good, small number.</span> </p><p><span m=''4445000''>So, I just need some separation
  between all of my parts.</span> </p><p><span m=''4448827''>And for this problem,
  I''m going to be pretty simple,</span> <span m=''4452378''>just say that the parts
  are only allowed to slide</span> <span m=''4455719''>horizontally. So, it''s a one-dimensional</span>
  <span m=''4458433''>problem. These objects are in 2-d,</span> <span m=''4460730''>or
  whatever, but I can only slide them an x</span> <span m=''4463654''>coordinate.
  So, to model that,</span> <span m=''4465672''>I''m going to look at the left edge
  of every part and say,</span> <span m=''4469570''>well, these two left edges should
  be at least some</span> <span m=''4472981''>separation. So, I think of it as whatever</span>
  <span m=''4476848''>the distance is plus some epsilon.</span> </p><p><span m=''4478952''>But,
  you know, if you have some funky 2-d</span> <span m=''4481501''>shapes you have
  to compute, well, this is a little bit too</span> <span m=''4485135''>close because
  these come into alignment.</span> </p><p><span m=''4487621''>But, there''s some
  constraint, well, for any two pieces,</span> <span m=''4491063''>I could figure
  out how close they can get.</span> </p><p><span m=''4493677''>They should get no
  closer. So, I''m going to call this x_1.</span> </p><p><span m=''4497309''>I''ll
  call this x_2. So, we have some constraint</span> <span m=''4500243''>like x_2 minus
  x_1 is at least d plus epsilon,</span> <span m=''4503111''>or whatever you compute
  that weight to be.</span> </p><p><span m=''4507000''>OK, so for every pair of pieces,
  I can do this,</span> <span m=''4509735''>compute some constraint on how far apart
  they have to be.</span> </p><p><span m=''4513066''>And, now I''d like to assign
  these x coordinates.</span> </p><p><span m=''4515861''>Right now, I''m assuming
  they''re just variables.</span> </p><p><span m=''4518596''>I want to slide these
  pieces around horizontally in order to</span> <span m=''4522105''>compactify them
  as much as possible so they fit in the</span> <span m=''4525257''>smallest chip
  that I can make because it costs money,</span> <span m=''4528350''>and time, and
  everything, and power, everything.</span> </p><p><span m=''4531145''>You always
  want your chip small.</span> </p><p><span m=''4534000''>So, Bellman-Ford does that.
  All right, so Bellman-Ford</span> <span m=''4540225''>solves these constraints because
  it''s just a bunch of difference</span> <span m=''4547626''>constraints. And we
  know that they are</span> <span m=''4551972''>solvable because you could spread
  all the pieces out</span> <span m=''4557963''>arbitrarily far. And, it minimizes
  the spread,</span> <span m=''4563250''>minimizes the size of the chip I need, a
  max of x_i minus the</span> <span m=''4570298''>min of x_i. So, this is it maximizes</span>
  <span m=''4574879''>compactness, or minimizes size of the chip.</span> </p><p><span
  m=''4578167''>OK, this is a one-dimensional problem, so it may seem a little</span>
  <span m=''4582943''>artificial, but the two dimensional problem is really</span>
  <span m=''4587014''>hard to solve. And this is,</span> <span m=''4589049''>in fact,
  the best you can do with a nice polynomial time</span> <span m=''4593355''>algorithm.
  There are other applications if</span> <span m=''4597419''>you''re scheduling events
  in, like, a multimedia environment,</span> <span m=''4602024''>and you want to guarantee
  that this audio plays at least two</span> <span m=''4606629''>seconds after this
  video, but then there are things that</span> <span m=''4610922''>are playing at
  the same time, and they have to be within some</span> <span m=''4615605''>gap of
  each other, so, lots of papers about using</span> <span m=''4619351''>Bellman-Ford,
  solve difference constraints to</span> <span m=''4622786''>enable multimedia environments.
  OK, so there you go.</span> </p><p><span m=''4626766''>And next class we''ll see
  more applications of Bellman-Ford to</span> <span m=''4631449''>all pairs shortest
  paths. Questions?</span> </p><p><span m=''4634181''>Great.</span> </p>'
type: course
uid: e942b39b3005c3e60aeafd8ef255efda

---
None