---
about_this_resource_text: '<p><strong>Topics covered</strong>:&nbsp;Shortest Paths
  III: All-pairs Shortest Paths, Matrix Multiplication, Floyd-Warshall, Johnson</p><p><strong>Instructors</strong>:&nbsp;Prof.
  Erik Demaine,&nbsp;Prof. Charles Leiserson</p>'
course_id: 6-046j-introduction-to-algorithms-sma-5503-fall-2005
embedded_media:
- id: lec19.pdf
  parent_uid: 2d2648ca3df8461c1d2d4ed58d52fd1e
  technical_location: https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-introduction-to-algorithms-sma-5503-fall-2005/video-lectures/lecture-19-shortest-paths-iii-all-pairs-shortest-paths-matrix-multiplication-floyd-warshall-johnson/lec19.pdf
  title: lec19.pdf
  type: null
  uid: 73130e7e79d3ba645da93604e084d436
- id: 6_046J_lec19_th.jpg
  parent_uid: 2d2648ca3df8461c1d2d4ed58d52fd1e
  technical_location: https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-introduction-to-algorithms-sma-5503-fall-2005/video-lectures/lecture-19-shortest-paths-iii-all-pairs-shortest-paths-matrix-multiplication-floyd-warshall-johnson/6_046J_lec19_th.jpg
  title: 6_046J_lec19_th.jpg
  type: null
  uid: 15c73395e0797f72fe301598c887193f
- id: Video-YouTube-Stream
  media_location: Sygq1e0xWnM
  parent_uid: 2d2648ca3df8461c1d2d4ed58d52fd1e
  title: Video-YouTube-Stream
  type: Video
  uid: 8c1ff4fbc2d6cb8f5a3b0ee43ced9ae1
- id: Thumbnail-YouTube-JPG
  media_location: https://img.youtube.com/vi/Sygq1e0xWnM/default.jpg
  parent_uid: 2d2648ca3df8461c1d2d4ed58d52fd1e
  title: Thumbnail-YouTube-JPG
  type: Thumbnail
  uid: 8d82fdb434b3929c6d09b029726fa203
- id: Video-iTunesU-MP4
  media_location: https://itunes.apple.com/us/itunes-u/id341597754
  parent_uid: 2d2648ca3df8461c1d2d4ed58d52fd1e
  title: Video-iTunes U-MP4
  type: Video
  uid: 5f630f14f9b7ab93fffb8b4dd5d72045
- id: Video-InternetArchive-MP4
  media_location: http://www.archive.org/download/MIT6.046JF05MPEG4/ocw-6.046-21nov2005-220k.mp4
  parent_uid: 2d2648ca3df8461c1d2d4ed58d52fd1e
  title: Video-Internet Archive-MP4
  type: Video
  uid: 2f82b3621e0c9dd673303216b280a361
- id: Video-iTunesU-MP3
  media_location: http://deimos3.apple.com/WebObjects/Core.woa/Browse/mit.edu.1298167185.01298167189.1303125300?i=1729911773
  parent_uid: 2d2648ca3df8461c1d2d4ed58d52fd1e
  title: Video-iTunes U-MP3
  type: Video
  uid: a192a84f1946bc2956685ab147a0e824
- id: Video-InternetArchive-MP3
  media_location: http://www.archive.org/download/MIT6.046JF05/ocw-6.046-21nov2005.mp3
  parent_uid: 2d2648ca3df8461c1d2d4ed58d52fd1e
  title: Video-Internet Archive-MP3
  type: Video
  uid: 701db92a0065a11f267cda5dff51f1c1
- id: Video-VideoLecturesnet-Stream
  media_location: http://videolectures.net/mit6046jf05_introduction_algorithms/
  parent_uid: 2d2648ca3df8461c1d2d4ed58d52fd1e
  title: Video-VideoLectures.net-Stream
  type: Video
  uid: 2a1809ca955573646eabea66d2e7d3ff
- id: Thumbnail-OCW-JPG
  parent_uid: 2d2648ca3df8461c1d2d4ed58d52fd1e
  title: Thumbnail-OCW-JPG
  type: Thumbnail
  uid: 68acde6743857a7d83edaeff6ac607f1
- id: 3Play-3PlayYouTubeid-MP4
  media_location: Sygq1e0xWnM
  parent_uid: 2d2648ca3df8461c1d2d4ed58d52fd1e
  title: 3Play-3Play YouTube id
  type: 3Play
  uid: b9221ad37d40afbaa9578b220645b2cd
- id: Sygq1e0xWnM.srt
  parent_uid: 2d2648ca3df8461c1d2d4ed58d52fd1e
  technical_location: https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-introduction-to-algorithms-sma-5503-fall-2005/video-lectures/lecture-19-shortest-paths-iii-all-pairs-shortest-paths-matrix-multiplication-floyd-warshall-johnson/Sygq1e0xWnM.srt
  title: 3play caption file
  type: null
  uid: 89952be0358a949c2e2323eb3bc589b9
- id: Sygq1e0xWnM.pdf
  parent_uid: 2d2648ca3df8461c1d2d4ed58d52fd1e
  technical_location: https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-introduction-to-algorithms-sma-5503-fall-2005/video-lectures/lecture-19-shortest-paths-iii-all-pairs-shortest-paths-matrix-multiplication-floyd-warshall-johnson/Sygq1e0xWnM.pdf
  title: 3play pdf file
  type: null
  uid: 6792814fc66b5436eb389cc6d114e882
- id: Caption-3Play YouTube id-SRT
  parent_uid: 2d2648ca3df8461c1d2d4ed58d52fd1e
  title: Caption-3Play YouTube id-SRT-English - US
  type: Caption
  uid: 1967b225272f0dde2fc2de6f8bf64574
- id: Transcript-3Play YouTube id-PDF
  parent_uid: 2d2648ca3df8461c1d2d4ed58d52fd1e
  title: Transcript-3Play YouTube id-PDF-English - US
  type: Transcript
  uid: 8c7bb7734eafce226927c686c6979e61
inline_embed_id: 71451661lecture19:shortestpathsiii:all-pairsshortestpaths,matrixmultiplication,floyd-warshall,johnson82758577
layout: video
order_index: null
parent_uid: c492612542f7cc7a09f73790a5f91d81
related_resources_text: <p>Lecture Notes (<a target="_blank" href="./resolveuid/73130e7e79d3ba645da93604e084d436">PDF</a>)<br
  />             <a target="_blank" href="./resolveuid/efc69ef86c18e164d675bd8808c6477a">Assignments</a><br
  />             <a target="_blank" href="./resolveuid/144d9e513546eac8c1fd9b0d278e6eb2">Exams</a></p>
short_url: lecture-19-shortest-paths-iii-all-pairs-shortest-paths-matrix-multiplication-floyd-warshall-johnson
technical_location: https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-introduction-to-algorithms-sma-5503-fall-2005/video-lectures/lecture-19-shortest-paths-iii-all-pairs-shortest-paths-matrix-multiplication-floyd-warshall-johnson
template_type: Tabbed
title: 'Lecture 19: Shortest Paths III: All-pairs Shortest Paths, Matrix Multiplication,
  Floyd-Warshall, Johnson'
transcript: '<p><span m=''7000''>-- shortest paths. This is the finale.</span> </p><p><span
  m=''10000''>Hopefully it was worth waiting for.</span> </p><p><span m=''13000''>Remind
  you there''s a quiz coming up soon,</span> <span m=''17000''>you should be studying
  for it. There''s no problem set due at</span> <span m=''23000''>the same time as
  the quiz because you should be studying</span> <span m=''28000''>now. It''s a take-home
  exam.</span> </p><p><span m=''32000''>It''s required that you come to class on Monday.</span>
  </p><p><span m=''37000''>Of course, you''ll all come, but everyone watching at home</span>
  <span m=''43000''>should also come next Monday to get the quiz.</span> </p><p><span
  m=''47000''>It''s the required lecture. So, we need a bit of a recap in</span> <span
  m=''53000''>the trilogy so far. So, the last two lectures,</span> <span m=''58000''>the
  last two episodes, or about single source shortest</span> <span m=''64000''>paths.
  So, we wanted to find the</span> <span m=''68000''>shortest path from a source vertex
  to every other vertex.</span> </p><p><span m=''73000''>And, we saw a few algorithms
  for this.</span> </p><p><span m=''77000''>Here''s some recap. We saw in the unweighted
  case,</span> <span m=''81000''>that was sort of the easiest where all the edge weights
  were</span> <span m=''87000''>one. Then we could use breadth first</span> <span
  m=''90000''>search. And this costs what we call</span> <span m=''94000''>linear
  time in the graph world, the number of vertices plus the</span> <span m=''101000''>number
  of edges. The next simplest case,</span> <span m=''106000''>perhaps, is nonnegative
  edge weights.</span> </p><p><span m=''110000''>And in that case, what algorithm
  do we use?</span> </p><p><span m=''114000''>Dijkstra, all right, everyone''s awake.</span>
  </p><p><span m=''120000''>Several answers at once, great.</span> </p><p><span m=''124000''>So
  this takes almost linear time if you use a good heap</span> <span m=''131000''>structure,
  so, V log V plus E.</span> </p><p><span m=''135000''>And, in the general case, general
  weights,</span> <span m=''141000''>we would use Bellman-Ford which you saw.</span>
  </p><p><span m=''146000''>And that costs VE, good, OK, which is quite a bit</span>
  <span m=''153000''>worse. This is ignoring log factors.</span> </p><p><span m=''158000''>Dijkstra
  is basically linear time, Bellman-Ford you''re</span> <span m=''162000''>quadratic
  if you have a connected graph.</span> </p><p><span m=''165000''>So, in the sparse
  case, when E is order V,</span> <span m=''169000''>this is about linear. This is
  about quadratic.</span> </p><p><span m=''172000''>In the dense case, when E is about
  V^2,</span> <span m=''176000''>this is quadratic, and this is cubic.</span> </p><p><span
  m=''180000''>So, Dijkstra and Bellman-Ford are separated by about an order</span>
  <span m=''186000''>of V factor, which is pretty bad.</span> </p><p><span m=''189000''>OK,
  but that''s the best we know how to do for single source</span> <span m=''195000''>shortest
  paths, negative edge weights,</span> <span m=''199000''>Bellman-Ford is the best.
  We also saw in recitation the</span> <span m=''204000''>case of a DAG. And there,
  what do you do?</span> </p><p><span m=''210000''>Topological sort, yeah.</span>
  </p><p><span m=''212000''>So, you can do a topological sort to get an ordering on
  the</span> <span m=''219000''>vertices. That you run Bellman-Ford,</span> <span
  m=''222000''>one round. This is one way to think of</span> <span m=''227000''>what''s
  going on. You run Bellman-Ford in the</span> <span m=''231000''>order given by the
  topological sort, which is once,</span> <span m=''237000''>and you get a linear
  time algorithm.</span> </p><p><span m=''243000''>So, DAG is another case where we
  know how to do well even with</span> <span m=''246000''>weights. Unweighted, we
  can also do</span> <span m=''248000''>linear time. But most of the time,</span>
  <span m=''250000''>though, will be, so you should keep these in</span> <span m=''253000''>mind
  in the quiz. When you get a shortest path</span> <span m=''255000''>problem, or
  what you end up determining is the shortest path</span> <span m=''259000''>problem,
  think about what''s the best algorithm you can use in</span> <span m=''262000''>that
  case? OK, so that''s single source</span> <span m=''264000''>shortest paths. And
  so, in our evolution of the</span> <span m=''267000''>Death Star, initially it was
  just nonnegative edge weights.</span> </p><p><span m=''270000''>Then we got negative
  edge weights.</span> </p><p><span m=''274000''>Today, the Death Star challenges
  us with all pair</span> <span m=''277000''>shortest paths, where we want to know
  the</span> <span m=''280000''>shortest path weight between every pair of vertices.</span>
  </p><p><span m=''299000''>OK, so let''s get some quick results.</span> </p><p><span
  m=''303000''>What could we do with this case?</span> </p><p><span m=''307000''>So,
  for example, suppose I have an unweighted</span> <span m=''313000''>graph. Any suggestions
  of how I should</span> <span m=''318000''>compute all pair shortest paths? Between
  every pair of vertices,</span> <span m=''326000''>I want to know the shortest path
  weight.</span> </p><p><span m=''332000''>BFS, a couple more words? Yeah?</span>
  </p><p><span m=''337000''>Right, BFS V times. OK, I''ll say V times BFS,</span>
  <span m=''344000''>OK? So, the running time would be</span> <span m=''349000''>V^2
  plus V times E, yeah, which is assuming your</span> <span m=''357000''>graph is
  connected, V times E.</span> </p><p><span m=''363000''>OK, good. That''s probably
  about the best</span> <span m=''365000''>algorithm we know for unweighted graphs.</span>
  </p><p><span m=''367000''>So, a lot of these are going to sort of be the obvious
  answer.</span> </p><p><span m=''371000''>You take your single source algorithm,
  you run it V times.</span> </p><p><span m=''375000''>That''s the best you can do,
  OK, or the best we know how to</span> <span m=''378000''>do. This is not so bad.</span>
  </p><p><span m=''379000''>This is like one iteration of Bellman-Ford,</span> <span
  m=''382000''>for comparison. We definitely need at least,</span> <span m=''385000''>like,
  V^2 time, because the size of the output</span> <span m=''387000''>is V^2, shortest
  path weight we have to compute.</span> </p><p><span m=''392000''>So, this is not
  perfect, but pretty good.</span> </p><p><span m=''397000''>And we are not going
  to improve on that.</span> </p><p><span m=''401000''>So, nonnegative edge weights:
  the natural thing to do is to</span> <span m=''409000''>run Dijkstra V times, OK,
  no big surprise.</span> </p><p><span m=''414000''>And the running time of that is,
  well, V times E again,</span> <span m=''421000''>plus V^2, log V, which is also
  not too bad.</span> </p><p><span m=''428000''>I mean, it''s basically the same as
  running BFS.</span> </p><p><span m=''430000''>And then, there''s the log factor.</span>
  </p><p><span m=''432000''>If you ignore the log factor, this is the dominant term.</span>
  </p><p><span m=''436000''>And, I mean, this had an [added?] V^2 as</span> <span
  m=''438000''>well. So, these are both pretty good.</span> </p><p><span m=''440000''>I
  mean, this is kind of neat. Essentially,</span> <span m=''442000''>the time it takes
  to run one Bellman-Ford plus a log factor,</span> <span m=''446000''>you can compute
  all pair shortest paths if you have</span> <span m=''449000''>nonnegative edge weights.
  So, I mean, comparing all pairs</span> <span m=''455000''>to signal source, this
  seems a lot better,</span> <span m=''459000''>except we can only handle nonnegative
  edge weights.</span> </p><p><span m=''465000''>OK, so now let''s think about the
  general case.</span> </p><p><span m=''469000''>Well, this is the focus of today,
  and here''s where we can</span> <span m=''475000''>actually make an improvement.
  So the obvious thing is V times</span> <span m=''482000''>Bellman-Ford, which would
  cost V^2 times E.</span> </p><p><span m=''488000''>And that''s pretty pitiful, and
  we''re going to try to</span> <span m=''491000''>improve that to something closer
  to that nonnegative edge weight</span> <span m=''495000''>bound. So it turns out,</span>
  <span m=''497000''>here, we can actually make an improvement whereas in these</span>
  <span m=''501000''>special cases, we really can''t do much better.</span> </p><p><span
  m=''504000''>OK, I don''t have a good intuition why,</span> <span m=''506000''>but
  it''s the case. So, we''ll cover something like</span> <span m=''510000''>three
  algorithms today for this problem.</span> </p><p><span m=''514000''>The last one
  will be the best, but along the way we''ll see</span> <span m=''517000''>some nice
  connections between shortest paths and dynamic</span> <span m=''520000''>programming,
  which we haven''t really seen yet.</span> </p><p><span m=''522000''>We''ve seen
  shortest path, and applying greedy algorithms</span> <span m=''526000''>to it, but
  today will actually do dynamic programming.</span> </p><p><span m=''529000''>The
  intuition is that with all pair shortest paths,</span> <span m=''531000''>there''s
  more potential subproblem reuse.</span> </p><p><span m=''534000''>We''ve got to
  compute the shortest path from x to y for</span> <span m=''537000''>all x and y.
  Maybe we can reuse those</span> <span m=''539000''>shortest paths in computing other
  shortest paths.</span> </p><p><span m=''543000''>OK, there''s a bit more reusability,
  let''s say.</span> </p><p><span m=''547000''>OK, let me quickly define all pair
  shortest paths formally,</span> <span m=''552000''>because we''re going to change
  our notation slightly.</span> </p><p><span m=''557000''>It''s because we care about
  all pairs.</span> </p><p><span m=''560000''>So, as usual, the input is directed
  graph,</span> <span m=''564000''>so, vertices and edges. We''re going to say that
  the</span> <span m=''569000''>vertices are labeled one to n for convenience because
  with all</span> <span m=''575000''>pairs, we''re going to think of things more as
  an n by n matrix</span> <span m=''582000''>instead of edges in some sense because
  it doesn''t help to think</span> <span m=''588000''>any more in terms of adjacency
  lists.</span> </p><p><span m=''591000''>And, you have edge weights as usual.</span>
  </p><p><span m=''595000''>This is what makes it interesting.</span> </p><p><span
  m=''600000''>Some of them are going to be negative.</span> </p><p><span m=''605000''>So,
  w maps to every real number, and the target output is</span> <span m=''613000''>a
  shortest path matrix. So, this is now an n by n</span> <span m=''620000''>matrix.
  So, n is just the number of</span> <span m=''625000''>vertices of shortest path
  weights.</span> </p><p><span m=''632000''>So, delta of i, j is the shortest path
  weight</span> <span m=''637000''>from i to j for all pairs of vertices.</span> </p><p><span
  m=''642000''>So this, you could represent as an n by n matrix in particular.</span>
  </p><p><span m=''650000''>OK, so now let''s start doing algorithms.</span> </p><p><span
  m=''657000''>So, we have this very simple algorithm, V times Bellman-Ford,</span>
  <span m=''662000''>V^2 times E, and just for comparison''s sake,</span> <span m=''666000''>I''m
  going to say, let me rewrite that,</span> <span m=''669000''>V times Bellman-Ford
  gives us this running time of V^2 E,</span> <span m=''674000''>and I''m going to
  think about the case where,</span> <span m=''678000''>let''s just say the graph
  is dense, meeting that the number</span> <span m=''683000''>of edges is quadratic,
  and the number of vertices.</span> </p><p><span m=''689000''>So in that case, this
  will take V^4 time,</span> <span m=''693000''>which is pretty slow. We''d like to
  do better.</span> </p><p><span m=''697000''>So, first goal would just be to beat
  V^4, V hypercubed,</span> <span m=''703000''>I guess. OK, and we are going to use</span>
  <span m=''706000''>dynamic programming to do that. Or at least that''s what the</span>
  <span m=''712000''>motivation will come from. It will take us a while before</span>
  <span m=''718000''>we can even beat V^4, which is maybe a bit pathetic,</span> <span
  m=''723000''>but it takes some clever insights, let''s say.</span> </p><p><span
  m=''730000''>OK, so I''m going to introduce a bit more notation for this</span>
  <span m=''739000''>graph. So, I''m going to think about</span> <span m=''745000''>the
  weighted adjacency matrix. So, I don''t think we''ve really</span> <span m=''753000''>seen
  this in lecture before, although I think it''s in the</span> <span m=''757000''>appendix.
  What that means,</span> <span m=''759000''>so normally adjacency matrix is like
  one if there''s an edge,</span> <span m=''764000''>and zero if there isn''t. And
  this is in a digraph,</span> <span m=''767000''>so you have to be a little bit careful.</span>
  </p><p><span m=''770000''>Here, these values, the entries in the matrix,</span>
  <span m=''774000''>are going to be the weights of the edges.</span> </p><p><span
  m=''777000''>OK, this is this if ij is an edge.</span> </p><p><span m=''781000''>So,
  if ij is an edge in the graph, and it''s going to be</span> <span m=''784000''>infinity
  if there is no edge. OK, in terms of shortest paths,</span> <span m=''788000''>this
  is a more useful way to represent the graph.</span> </p><p><span m=''792000''>All
  right, and so this includes everything that we need from</span> <span m=''796000''>here.
  And now we just have to think</span> <span m=''798000''>about it as a matrix. Matrices
  will be a useful tool</span> <span m=''801000''>in a little while. OK, so now I''m
  going to define</span> <span m=''805000''>some sub problems. And, there''s different
  ways</span> <span m=''808000''>that you could define what''s going on in the shortest
  paths</span> <span m=''812000''>problem. OK, the natural thing is I want</span>
  <span m=''815000''>to go from vertex i to vertex j. What''s the shortest path?</span>
  </p><p><span m=''819000''>OK, we need to refine the sub problems a little but more
  than</span> <span m=''822000''>that. Not surprising.</span> </p><p><span m=''823000''>And
  if you think about my analogy to Bellman-Ford,</span> <span m=''826000''>what Bellman-Ford
  does is it tries to build longer and longer</span> <span m=''830000''>shortest paths.
  But here, length is in terms of</span> <span m=''832000''>the number of edges. So,
  first, it builds shortest</span> <span m=''835000''>paths of length one. We''ve
  proven the first round it</span> <span m=''838000''>does that. The second round,</span>
  <span m=''841000''>it provides all shortest paths of length two,</span> <span m=''846000''>of
  count two, and so on.</span> </p><p><span m=''848000''>We''d like to do that sort
  of analogously, and try to reuse</span> <span m=''854000''>things a little bit more.
  So, I''m going to say d_ij^(m)</span> <span m=''860000''>is the weight of the shortest
  path from i to j with some</span> <span m=''866000''>restriction involving m. So:
  shortest path from i to j</span> <span m=''873000''>using at most m edges. OK, for
  example,</span> <span m=''876000''>if m is zero, then we don''t have to really</span>
  <span m=''881000''>think very hard to find all shortest paths of length zero.</span>
  </p><p><span m=''887000''>OK, they use zero edges, I should say.</span> </p><p><span
  m=''890000''>So, Bellman-Ford sort of tells us how to go from m to m plus</span>
  <span m=''897000''>one. So, let''s just figure that out.</span> </p><p><span m=''902000''>So
  one thing we know from the Bellman-Ford analysis is if we</span> <span m=''905000''>look
  at d_ij^(m-1), we know that in some sense the</span> <span m=''908000''>longest
  shortest path of relevance, unless you have</span> <span m=''912000''>negative weight
  cycle, the longest shortest path of</span> <span m=''915000''>relevance is when
  m equals n minus one because that''s the</span> <span m=''919000''>longest simple
  path you can have.</span> </p><p><span m=''921000''>So, this should be a shortest
  path weight from i to j,</span> <span m=''924000''>and it would be no matter what
  larger value you put in the</span> <span m=''928000''>superscript. This should be
  delta of i comma</span> <span m=''932000''>j if there''s no negative weight cycles.</span>
  </p><p><span m=''935000''>OK, so this feels good for dynamic programming.</span>
  </p><p><span m=''938000''>This will give us the answer if we can compute this for
  all m.</span> </p><p><span m=''943000''>Then we''ll have the shortest path weights
  in particular.</span> </p><p><span m=''947000''>We need a way to detect negative
  weight cycles,</span> <span m=''950000''>but let''s not worry about that too much
  for now.</span> </p><p><span m=''954000''>There are negative weights, but let''s
  just assume for now</span> <span m=''958000''>there''s no negative weight cycles.</span>
  </p><p><span m=''962000''>OK, and we get a recursion recurrence.</span> </p><p><span
  m=''966000''>And the base case is when m equals zero.</span> </p><p><span m=''970000''>This
  is pretty easy. They have the same vertices,</span> <span m=''976000''>the weight
  of zero, and otherwise it''s infinity.</span> </p><p><span m=''982000''>OK, and
  then the actual recursion is for m.</span> </p><p><span m=''1017000''>OK, if I got
  this right, this is a pretty easy,</span> <span m=''1020000''>intuitive recursion
  for d_ij^(m) is a min of smaller</span> <span m=''1025000''>things in terms of n
  minus one. I''ll just show the picture,</span> <span m=''1030000''>and then the
  proof of that claim should be obvious.</span> </p><p><span m=''1034000''>So, this
  is proof by picture. So, we have on the one hand,</span> <span m=''1039000''>I over
  here, and j over here.</span> </p><p><span m=''1042000''>We want to know the shortest
  path from i to j.</span> </p><p><span m=''1045000''>And, we want to use, at most,
  m edges.</span> </p><p><span m=''1050000''>So, the idea is, well, you could use
  m minus one</span> <span m=''1054000''>edges to get somewhere. So this is, at most,</span>
  <span m=''1059000''>m minus one edges, some other place,</span> <span m=''1062000''>and
  we''ll call it k. So this is a candidate for k.</span> </p><p><span m=''1068000''>And
  then you could take the edge directly from k to j.</span> </p><p><span m=''1073000''>So,
  this costs A_k^j, and this costs DIK m minus one.</span> </p><p><span m=''1080000''>OK,
  and that''s a candidate path of length that uses,</span> <span m=''1082000''>at
  most, m edges from I to j. And this is essentially just</span> <span m=''1086000''>considering
  all of them. OK, so there''s sort of many</span> <span m=''1088000''>paths we are
  considering. All of these are candidate</span> <span m=''1091000''>values of k.
  We are taking them in over all</span> <span m=''1094000''>k as intermediate nodes,
  whatever.</span> </p><p><span m=''1096000''>So there they are. We take the best
  such path.</span> </p><p><span m=''1098000''>That should encompass all shortest
  paths.</span> </p><p><span m=''1100000''>And this is essentially sort of what Bellman-Ford
  is doing,</span> <span m=''1104000''>although not exactly. We also sort of want
  to think</span> <span m=''1106000''>about, well, what if I just go directly with,</span>
  <span m=''1109000''>say, m minus one edges? What if there is no edge here</span>
  <span m=''1114000''>that I want to use, in some sense?</span> </p><p><span m=''1116000''>Well,
  we always think about there being, and the way the A''s</span> <span m=''1120000''>are
  defined, there''s always this zero weight edge to yourself.</span> </p><p><span
  m=''1125000''>So, you could just take a path that''s shorter,</span> <span m=''1128000''>go
  from d i to j, and j is a particular value of</span> <span m=''1131000''>k that
  we might consider, and then take a zero weight</span> <span m=''1135000''>edge at
  the end from A and jj. OK, so this really encompasses</span> <span m=''1140000''>everything.
  So that''s a pretty trivial</span> <span m=''1143000''>claim. OK, now once we have
  such a</span> <span m=''1146000''>recursion, we get a dynamic program.</span> </p><p><span
  m=''1148000''>I mean, there, this is it in some sense.</span> </p><p><span m=''1151000''>It''s
  written recursively. You can write a bottom up.</span> </p><p><span m=''1155000''>And
  I would like to write it bottom up it little bit because</span> <span m=''1159000''>while
  it doesn''t look like it, this is a relaxation.</span> </p><p><span m=''1163000''>This
  is yet another relaxation algorithm.</span> </p><p><span m=''1166000''>So, I''ll
  give you, so, this is sort of the</span> <span m=''1169000''>algorithm. This is
  not a very interesting</span> <span m=''1171000''>algorithm. So, you don''t have
  to write it</span> <span m=''1175000''>all down if you don''t feel like it.</span>
  </p><p><span m=''1178000''>It''s probably not even in the book.</span> </p><p><span
  m=''1180000''>This is just an intermediate step.</span> </p><p><span m=''1182000''>So,
  we loop over all m. That''s sort of the outermost</span> <span m=''1185000''>thing
  to do. I want to build longer and</span> <span m=''1188000''>longer paths, and this
  vaguely corresponds to</span> <span m=''1191000''>Bellman-Ford, although it''s actually
  worse</span> <span m=''1193000''>than Bellman-Ford. But hey, what the heck?</span>
  </p><p><span m=''1196000''>It''s a stepping stone. OK, then for all i and j,</span>
  <span m=''1203000''>and then we want to compute this min.</span> </p><p><span m=''1210000''>So,
  we''ll just loop over all k, and relax.</span> </p><p><span m=''1217000''>And, here''s
  where we''re actually computing the min.</span> </p><p><span m=''1226000''>And,
  it''s a relaxation, is the point.</span> </p><p><span m=''1235000''>This is our
  good friend, the relaxation step,</span> <span m=''1238000''>relaxing edge. Well,
  it''s not,</span> <span m=''1240000''>yeah. I guess we''re relaxing edge kj,</span>
  <span m=''1242000''>or something, except we don''t have the same</span> <span m=''1245000''>clear
  notion. I mean, it''s a particular thing</span> <span m=''1248000''>that we''re
  relaxing. It''s not just a single edge</span> <span m=''1252000''>because we don''t
  have a single source anymore.</span> </p><p><span m=''1255000''>It''s now relative
  to source I, we are relaxing the edge kj,</span> <span m=''1259000''>something like
  that. But this is clearly a</span> <span m=''1263000''>relaxation. We are just making
  the triangle</span> <span m=''1265000''>inequality true if it wasn''t before.</span>
  </p><p><span m=''1268000''>The tribal inequality has got to hold between all pairs.</span>
  </p><p><span m=''1271000''>And that''s just implementing this min, right?</span>
  </p><p><span m=''1274000''>You''re taking d ij. You take the min of what it was</span>
  <span m=''1277000''>before in some sense. That was one of the</span> <span m=''1279000''>possibilities
  we considered when we looked at the zero weight</span> <span m=''1283000''>edge.
  We say, well,</span> <span m=''1284000''>or you could go from i to some k in some
  way that we knew how</span> <span m=''1288000''>to before, and then add on the edge,
  and check whether that''s</span> <span m=''1292000''>better if it''s better, set
  our current estimate to</span> <span m=''1295000''>that. And, you do this for all
  k.</span> </p><p><span m=''1298000''>In particular, you might actually compute</span>
  <span m=''1300000''>something smaller than this min because I didn''t put</span>
  <span m=''1303000''>superscripts up here. But that''s just making paths</span> <span
  m=''1306000''>even better. OK, so you have to argue that</span> <span m=''1309000''>relaxation
  is always a good thing to do.</span> </p><p><span m=''1311000''>So, by not putting
  superscripts,</span> <span m=''1313000''>maybe I do some more relaxation, but more
  relaxation</span> <span m=''1316000''>never hurts us. You can still argue correctness</span>
  <span m=''1319000''>using this claim. So, it''s not quite the direct</span> <span
  m=''1323000''>implementation, but there you go,</span> <span m=''1325000''>dynamic
  programming algorithm. The main reason I''ll write it</span> <span m=''1330000''>down:
  so you see that it''s a relaxation, and you see the</span> <span m=''1334000''>running
  time is n^4, OK, which is certainly no</span> <span m=''1338000''>better than Bellman-Ford.
  Bellman-Ford was n^4 even in</span> <span m=''1342000''>the dense case, and it''s
  a little better in the</span> <span m=''1346000''>sparse case. So: not doing so
  great.</span> </p><p><span m=''1350000''>But it''s a start. OK, it gets our dynamic</span>
  <span m=''1354000''>programming minds thinking. And, we''ll get a better dynamic</span>
  <span m=''1361000''>program in a moment. But first, there''s actually</span> <span
  m=''1367000''>something useful we can do with this formulation,</span> <span m=''1372000''>and
  I guess I''ll ask, but I''ll be really impressed if</span> <span m=''1379000''>anyone
  can see. Does this formula look like</span> <span m=''1384000''>anything else that
  you''ve seen in any context,</span> <span m=''1389000''>mathematical or algorithmic?
  Have you seen that recurrence</span> <span m=''1395000''>anywhere else? OK, not
  exactly as stated,</span> <span m=''1400000''>but similar. I''m sure if you thought
  about</span> <span m=''1404000''>it for awhile, you could come up with it.</span>
  </p><p><span m=''1410000''>Any answers? I didn''t think you would be</span> <span
  m=''1413000''>very intuitive, but the answer is matrix</span> <span m=''1416000''>multiplication.
  And it may now be obvious to</span> <span m=''1419000''>you, or it may not. You
  have to think with the</span> <span m=''1423000''>right quirky mind. Then it''s
  obvious that it''s</span> <span m=''1427000''>matrix multiplication. Remember, matrix</span>
  <span m=''1430000''>multiplication, we have A, B,</span> <span m=''1432000''>and
  C. They''re all n by n matrices.</span> </p><p><span m=''1435000''>And, we want
  to compute C equals A times B.</span> </p><p><span m=''1440000''>And what that meant
  was, well, c_ij was a sum over all k</span> <span m=''1444000''>of a_ik times b_kj.
  All right, that was our</span> <span m=''1448000''>definition of matrix multiplication.</span>
  </p><p><span m=''1451000''>And that formula looks kind of like this one.</span>
  </p><p><span m=''1455000''>I mean, notice the subscripts: ik and kj.</span> </p><p><span
  m=''1459000''>Now, the operators are a little different.</span> </p><p><span m=''1462000''>Here,
  we''re multiplying the inside things and adding them</span> <span m=''1467000''>all
  together. There, we''re adding the inside</span> <span m=''1474000''>things and
  taking them in. But other than that,</span> <span m=''1481000''>it''s the same.
  OK, weird, but here we go.</span> </p><p><span m=''1487000''>So, the connection
  to shortest paths is you replace these</span> <span m=''1495000''>operators. So,
  let''s take matrix</span> <span m=''1500000''>multiplication and replace, what should
  I do first,</span> <span m=''1505000''>plus this thing with min. So, why not just
  change the</span> <span m=''1510000''>operators, replace dot with plus?</span> </p><p><span
  m=''1513000''>This is just a different algebra to work in,</span> <span m=''1518000''>where
  plus actually means min, and dot actually means plus.</span> </p><p><span m=''1523000''>So,
  you have to check that things sort of work out in that</span> <span m=''1529000''>context,
  but if we do that, then we get that c_ij is the</span> <span m=''1535000''>min overall
  k of a_ik plus, a bit messy here,</span> <span m=''1539000''>b_kj. And that looks
  like what we</span> <span m=''1544000''>actually want to compute, here, for one
  value of m,</span> <span m=''1549000''>you have to sort of do this m times.</span>
  </p><p><span m=''1552000''>But this conceptually is d_ij^(m), and this is</span>
  <span m=''1556000''>d_ik^(m-1). So, this is looking like a</span> <span m=''1559000''>matrix
  product, which is kind of cool.</span> </p><p><span m=''1564000''>So, if we sort
  of plug in this claim, then, and think about</span> <span m=''1571000''>things as
  matrices, the recurrence gives us,</span> <span m=''1577000''>and I''ll just write
  this now at matrix form, that d^(m) is d^(m)</span> <span m=''1585000''>minus one,
  funny product, A.</span> </p><p><span m=''1590000''>All right, so these are the
  weights.</span> </p><p><span m=''1592000''>These were the weighted adjacency matrix.</span>
  </p><p><span m=''1594000''>This was the previous d value. This is the new d value.</span>
  </p><p><span m=''1598000''>So, I''ll just rewrite that in matrix form with capital</span>
  <span m=''1601000''>letters. OK, I have the circle up things</span> <span m=''1603000''>that
  are using this funny algebra, so, in particular,</span> <span m=''1607000''>circled
  product. OK, so that''s kind of nifty.</span> </p><p><span m=''1609000''>We know
  something about computing matrix</span> <span m=''1612000''>multiplications. We
  can do it in n^3 time.</span> </p><p><span m=''1614000''>If we were a bit fancier,
  maybe we could do it in</span> <span m=''1617000''>sub-cubic time. So, we could
  try to sort of use</span> <span m=''1622000''>this connection. And, well, think
  about what we</span> <span m=''1627000''>are computing here. We are saying,</span>
  <span m=''1630000''>well, d to the m is the previous one times A.</span> </p><p><span
  m=''1634000''>So, what is d^(m)? Is that some other algebraic</span> <span m=''1639000''>notion
  that we know? Yeah, it''s the exponent.</span> </p><p><span m=''1643000''>We''re
  taking A, and we want to raise it to the</span> <span m=''1647000''>power, m, with
  this funny notion of product.</span> </p><p><span m=''1653000''>So, in other words,
  d to the m is really just A to</span> <span m=''1656000''>the m in a funny way.
  So, I''ll circle it,</span> <span m=''1660000''>OK? So, that sounds good.</span>
  </p><p><span m=''1661000''>We also know how to compute powers of things relatively</span>
  <span m=''1666000''>quickly, if you remember how. OK, for this notion,</span> <span
  m=''1670000''>this power notion, to make sense,</span> <span m=''1672000''>I should
  say what A to the zero means.</span> </p><p><span m=''1675000''>And so, I need some
  kind of identity matrix.</span> </p><p><span m=''1680000''>And for here, the identity
  matrix is this</span> <span m=''1682000''>one, if I get it right. So, it has zeros
  along the</span> <span m=''1686000''>diagonal, and infinities everywhere else.</span>
  </p><p><span m=''1689000''>OK, that sort of just to match this definition.</span>
  </p><p><span m=''1692000''>d_ij zero should be zeros on the diagonals and infinity</span>
  <span m=''1696000''>everywhere else. But you can check this is</span> <span m=''1699000''>actually
  an identity. If you multiply it with this</span> <span m=''1703000''>funny multiplication
  against any other matrix,</span> <span m=''1706000''>you get the matrix back. Nothing
  changes.</span> </p><p><span m=''1711000''>This really is a valid identity matrix.</span>
  </p><p><span m=''1714000''>And, I should mention that for A to the m to make sense,</span>
  <span m=''1720000''>you really knew that your product operation is</span> <span
  m=''1724000''>associative. So, actually A to the m circled</span> <span m=''1728000''>makes
  sense because circled multiplication is associative,</span> <span m=''1734000''>and
  you can check that; not hard because,</span> <span m=''1738000''>I mean, min is
  associative, and addition is associative,</span> <span m=''1743000''>and all sorts
  of good stuff. And, you have some kind of</span> <span m=''1750000''>distributivity
  property. And, this is,</span> <span m=''1754000''>in turn, because the real numbers
  with,</span> <span m=''1758000''>and get the right order here, with min as your
  addition</span> <span m=''1763000''>operation, and plus as your multiplication operation
  is a</span> <span m=''1769000''>closed semi-ring. So, if ever you want to know</span>
  <span m=''1774000''>when powers make sense, this is a good rule.</span> </p><p><span
  m=''1777000''>If you have a closed semi-ring, then matrix products on that</span>
  <span m=''1782000''>semi-ring will give you an associative operator,</span> <span
  m=''1786000''>and then, good, you can take products.</span> </p><p><span m=''1789000''>OK,
  that''s just some formalism. So now, we have some intuition.</span> </p><p><span
  m=''1794000''>The question is, what''s the right.</span> </p><p><span m=''1797000''>Algorithm?
  There are many possible</span> <span m=''1800000''>answers, some of which are right,
  some of which are not.</span> </p><p><span m=''1806000''>So, we have this connection
  to matrix products,</span> <span m=''1809000''>and we have a connection to matrix
  powers.</span> </p><p><span m=''1813000''>And, we have algorithms for both.</span>
  </p><p><span m=''1815000''>The question is, what should we do?</span> </p><p><span
  m=''1818000''>So, all we need to do now is to compute A to the funny power,</span>
  <span m=''1823000''>n minus one. n minus one is when we get</span> <span m=''1826000''>shortest
  paths, assuming we have no negative</span> <span m=''1829000''>weight cycles. In
  fact, we could compute a</span> <span m=''1834000''>larger power than n minus one.
  Once you get beyond n minus</span> <span m=''1839000''>one, multipling by A doesn''t
  change you anymore.</span> </p><p><span m=''1843000''>So, how should we do it? OK,
  you''re not giving any smart</span> <span m=''1847000''>answers. I''ll give the
  stupid answer.</span> </p><p><span m=''1850000''>You could say, well, I take A.</span>
  </p><p><span m=''1853000''>I multiply it by A. Then I multiply it by A,</span> <span
  m=''1856000''>and I multiply it by A, and I use normal,</span> <span m=''1860000''>boring
  matrix to multiplication.</span> </p><p><span m=''1864000''>So, I do, like, n minus
  two,</span> <span m=''1867000''>standard matrix multiplies. So, standard multiply
  costs,</span> <span m=''1873000''>like, n^3. And I''m doing n of them.</span> </p><p><span
  m=''1877000''>So, this gives me an n^4 algorithm, and compute all the</span> <span
  m=''1883000''>shortest pathways in n^4. Woohoo!</span> </p><p><span m=''1886000''>OK,
  no improvement. So, how can I do better?</span> </p><p><span m=''1891000''>Right,
  natural thing to try which sadly does not work,</span> <span m=''1896000''>is to
  use the sub cubic matrix multiply algorithm.</span> </p><p><span m=''1900000''>We
  will, in some sense, get there in a moment with a</span> <span m=''1904000''>somewhat
  simpler problem. But, it''s actually not known</span> <span m=''1908000''>how to
  compute shortest paths using fast matrix multiplication</span> <span m=''1913000''>like
  Strassen''s system algorithm.</span> </p><p><span m=''1915000''>But, good suggestion.
  OK, you have to think about why</span> <span m=''1920000''>it doesn''t work, and
  I''ll tell you.</span> </p><p><span m=''1924000''>It''s not obvious, so it''s a
  perfectly reasonable</span> <span m=''1927000''>suggestion. But in this context
  it doesn''t</span> <span m=''1930000''>quite work. It will come up in a few</span>
  <span m=''1932000''>moments. The problem is,</span> <span m=''1934000''>Strassen
  requires the notion of subtraction.</span> </p><p><span m=''1937000''>And here,
  addition is min. And, there''s no inverse to min.</span> </p><p><span m=''1941000''>Once
  you take the arguments, you can''t sort of undo a min.</span> </p><p><span m=''1945000''>OK,
  so there''s no notion of subtraction, so it''s not known</span> <span m=''1948000''>how
  to pull that off, sadly.</span> </p><p><span m=''1952000''>So, what other tricks
  do we have up our sleeve?</span> </p><p><span m=''1955000''>Yeah? Divide and conquer,</span>
  <span m=''1957000''>log n powering, yeah, repeated squaring.</span> </p><p><span
  m=''1961000''>That works. Good, we had a fancy way.</span> </p><p><span m=''1964000''>If
  you had a number n, you sort of looked at the</span> <span m=''1967000''>binary
  number representation of n, and you either squared the</span> <span m=''1972000''>number
  or squared it and added another factor of A.</span> </p><p><span m=''1977000''>Here,
  we don''t even have to be smart about it.</span> </p><p><span m=''1982000''>OK,
  we can just compute, we really only have to think</span> <span m=''1987000''>about
  powers of two. What we want to know,</span> <span m=''1991000''>and I''m going to
  need a bigger font here because there''s</span> <span m=''1997000''>multiple levels
  of subscripts, A to the circled power,</span> <span m=''2002000''>two to the ceiling
  of log n. Actually, n minus one would be</span> <span m=''2008000''>enough. But
  there you go.</span> </p><p><span m=''2012000''>You can write n if you didn''t leave
  yourself enough space like</span> <span m=''2015000''>me, n the ceiling, n the circle.</span>
  </p><p><span m=''2017000''>This just means the next power of two after n minus one,</span>
  <span m=''2021000''>two to the ceiling log. So, we don''t have to go</span> <span
  m=''2024000''>directly to n minus one. We can go further because</span> <span m=''2027000''>anything
  farther than n minus one is still just the shortest</span> <span m=''2031000''>pathways.
  If you look at the definition,</span> <span m=''2033000''>and you know that your
  paths are simple, which is true if you</span> <span m=''2037000''>have no negative
  weight cycles, then fine, just go farther.</span> </p><p><span m=''2042000''>Why
  not? And so, to compute this,</span> <span m=''2044000''>we just do ceiling of log
  n minus one products,</span> <span m=''2049000''>just take A squared, and then take
  the result and</span> <span m=''2053000''>square it; take the result and square
  it.</span> </p><p><span m=''2057000''>So, this is order log n squares.</span> </p><p><span
  m=''2060000''>And, we don''t know how to use Strassen, but we can use the</span>
  <span m=''2065000''>boring, standard multiply of n^3, and that gives us n^3 log
  n</span> <span m=''2070000''>running time, OK, which finally is something</span>
  <span m=''2074000''>that beats Bellman-Ford in the dense case.</span> </p><p><span
  m=''2080000''>OK, in the dense case, Bellman-Ford was n^4.</span> </p><p><span m=''2083000''>Here
  we get n^3 log n, finally something better.</span> </p><p><span m=''2086000''>In
  the sparse case, it''s about the same,</span> <span m=''2089000''>maybe a little
  worse. E is order V.</span> </p><p><span m=''2092000''>Then we''re going to get,
  like, V3 for Bellman-Ford.</span> </p><p><span m=''2095000''>Here, we get n^3 log
  n. OK, after log factors,</span> <span m=''2099000''>this is an improvement some
  of the time.</span> </p><p><span m=''2103000''>OK, it''s about the same the other
  times.</span> </p><p><span m=''2105000''>Another nifty thing that you get for free
  out of this,</span> <span m=''2109000''>is you can detect negative weight cycles.</span>
  </p><p><span m=''2113000''>So, here''s a bit of a puzzle. How would I detect,</span>
  <span m=''2116000''>after I compute this product, A to the power to ceiling log
  n</span> <span m=''2121000''>minus one, how would I know if I found a negative weight
  cycle?</span> </p><p><span m=''2125000''>What would that mean it this matrix of
  all their shortest</span> <span m=''2130000''>paths of, at most, a certain length?</span>
  </p><p><span m=''2134000''>If I found a cycle, what would have to be in that</span>
  <span m=''2136000''>matrix? Yeah?</span> </p><p><span m=''2137000''>Right, so I
  could, for example,</span> <span m=''2139000''>take this thing, multiply it by A,</span>
  <span m=''2141000''>see if the matrix changed at all.</span> </p><p><span m=''2143000''>Right,
  that works fine. That''s what we do in</span> <span m=''2145000''>Bellman-Ford.
  It''s an even simpler thing.</span> </p><p><span m=''2148000''>It''s already there.
  You don''t have to multiply.</span> </p><p><span m=''2151000''>But that''s the same
  running time.</span> </p><p><span m=''2152000''>That''s a good answer. The diagonal
  would have a</span> <span m=''2155000''>negative value, yeah.</span> </p><p><span
  m=''2156000''>So, this is just a cute thing. Both approaches would work,</span>
  <span m=''2164000''>can detect a negative weight cycle just by looking at the</span>
  <span m=''2175000''>diagonal of the matrix. You just look for a negative</span>
  <span m=''2184000''>value in the diagonal. OK.</span> </p><p><span m=''2190000''>So,
  that''s algorithm one, let''s say.</span> </p><p><span m=''2192000''>I mean, we''ve
  seen several that are all bad, but I''ll call this</span> <span m=''2197000''>number
  one. OK, we''ll see two more.</span> </p><p><span m=''2199000''>This is the only
  one that will, well, I shouldn''t say that.</span> </p><p><span m=''2204000''>Fine,
  there we go. So, this is one dynamic program</span> <span m=''2207000''>that wasn''t
  so helpful, except it showed us a</span> <span m=''2211000''>connection to matrix
  multiplication,</span> <span m=''2213000''>which is interesting. We''ll see why
  it''s useful a</span> <span m=''2217000''>little bit more. But, it bled to this
  nasty four</span> <span m=''2222000''>nested loops. And, using this trick,</span>
  <span m=''2224000''>we got down to n^3 log n. Let''s try, just for n^3.</span> </p><p><span
  m=''2228000''>OK, just get rid of that log. It''s annoying.</span> </p><p><span
  m=''2231000''>It makes you a little bit worse than Bellman-Ford,</span> <span m=''2235000''>and
  the sparse case. So, let''s just erase one of</span> <span m=''2238000''>these nested
  loops. OK, I want to do that.</span> </p><p><span m=''2241000''>OK, obviously that
  algorithm doesn''t work because it''s for</span> <span m=''2245000''>first decay,
  and it''s not defined, but,</span> <span m=''2248000''>you know, I''ve got enough
  variables.</span> </p><p><span m=''2251000''>Why don''t I just define k to the m?</span>
  </p><p><span m=''2255000''>OK, it turns out that works. I''ll do it from scratch,</span>
  <span m=''2259000''>but why not? I don''t know if that''s how</span> <span m=''2262000''>Floyd
  and Warshall came up with their algorithm,</span> <span m=''2267000''>but here you
  go. Here''s Floyd-Warshall.</span> </p><p><span m=''2270000''>The idea is to define
  the subproblems a little bit more</span> <span m=''2275000''>cleverly so that to
  compute one of these values,</span> <span m=''2279000''>you don''t have to take
  the min of n things.</span> </p><p><span m=''2284000''>I just want to take the min
  of two things.</span> </p><p><span m=''2286000''>If I could do that, and I still
  only have n^3</span> <span m=''2289000''>subproblems, then I would have n^3 time.</span>
  </p><p><span m=''2292000''>So, all right, the running time of dynamic</span> <span
  m=''2294000''>program is number of subproblems times the time to compute the</span>
  <span m=''2298000''>recurrence for one subproblem. So, here''s linear times n^3,</span>
  <span m=''2302000''>and we want n^3 times constant. That would be good.</span> </p><p><span
  m=''2306000''>So that''s Floyd-Warshall. So, here''s the way we''re going</span>
  <span m=''2309000''>to redefine c_ij. Or I guess, there it was called</span> <span
  m=''2315000''>d_ij. Good, so we''re going to define</span> <span m=''2319000''>something
  new. So, c_ij superscript k is now</span> <span m=''2323000''>going to be the weight
  of the shortest path from I to j as</span> <span m=''2330000''>before. Notice I
  used the superscript k</span> <span m=''2334000''>instead of m because I want k
  and m to be the same thing.</span> </p><p><span m=''2340000''>Deep. OK, now, here''s
  the new</span> <span m=''2343000''>constraint. I want all intermediate</span> <span
  m=''2345000''>vertices along the path, meeting all vertices except for</span> <span
  m=''2349000''>I and j at the beginning and the end to have a small label.</span>
  </p><p><span m=''2353000''>So, they should be in the set from one up to k.</span>
  </p><p><span m=''2357000''>And this is where we are really using that our vertices
  are</span> <span m=''2361000''>labeled one up to m. So, I''m going to say,</span>
  <span m=''2364000''>well, first think about the shortest paths that don''t use</span>
  <span m=''2368000''>any other vertices. That''s when k is zero.</span> </p><p><span
  m=''2372000''>Then think about all the shortest paths that maybe they</span> <span
  m=''2375000''>use vertex one. And then think about the</span> <span m=''2378000''>shortest
  paths that maybe use vertex one or vertex two.</span> </p><p><span m=''2381000''>Why
  not? You could define it in this</span> <span m=''2383000''>way. It turns out,</span>
  <span m=''2384000''>then when you increase k, you only have to think about</span>
  <span m=''2388000''>one new vertex. Here, we had to take min over</span> <span m=''2391000''>all
  k. Now we know which k to look at.</span> </p><p><span m=''2393000''>OK, maybe that
  made sense. Maybe it''s not quite obvious</span> <span m=''2397000''>yet. But I''m
  going to redo this</span> <span m=''2399000''>claim, redo a recurrence. So, maybe
  first I should say</span> <span m=''2404000''>some obvious things. So, if I want
  delta of ij of</span> <span m=''2407000''>the shortest pathway, well, just take
  all the</span> <span m=''2410000''>vertices. So, take c_ij superscript n.</span>
  </p><p><span m=''2413000''>That''s everything. And this even works,</span> <span
  m=''2415000''>this is true even if you have a negative weight cycle.</span> </p><p><span
  m=''2419000''>Although, again, we''re going to sort of ignore</span> <span m=''2422000''>negative
  weight cycles as long as we can detect them.</span> </p><p><span m=''2426000''>And,
  another simple case is if you have, well,</span> <span m=''2429000''>c_ij to zero.
  Let me put that in the claim to</span> <span m=''2435000''>be a little bit more
  consistent here.</span> </p><p><span m=''2440000''>So, here''s the new claim. If
  we want to compute c_ij</span> <span m=''2447000''>superscript zero, what is it?</span>
  </p><p><span m=''2450000''>Superscript zero means I really shouldn''t use any intermediate</span>
  <span m=''2458000''>vertices. So, this has a very simple</span> <span m=''2463000''>answer,
  a three letter answer. So, it''s not zero.</span> </p><p><span m=''2469000''>It''s
  four letters. What''s that?</span> </p><p><span m=''2472000''>Nil. No, not working
  yet.</span> </p><p><span m=''2475000''>It has some subscripts, too.</span> </p><p><span
  m=''2478000''>So, the definition would be, what''s the shortest path weight</span>
  <span m=''2485000''>from I to j when you''re not allowed to use any intermediate</span>
  <span m=''2491000''>vertices? Sorry?</span> </p><p><span m=''2494000''>So, yeah,
  it has a very simple name.</span> </p><p><span m=''2498000''>That''s the tricky
  part. All right, so if i equals j,</span> <span m=''2503000''>[LAUGHTER] you''re
  clever, right, open bracket i equals j</span> <span m=''2508000''>means one, well,
  OK.</span> </p><p><span m=''2510000''>It sort of works, but it''s not quite right.</span>
  </p><p><span m=''2514000''>In fact, I want infinity if i does not equal j.</span>
  </p><p><span m=''2519000''>And I want to zero if i equals j, a_ij, good.</span>
  </p><p><span m=''2525000''>I think it''s a_ij. It should be,</span> <span m=''2527000''>right?
  Maybe I''m wrong.</span> </p><p><span m=''2529000''>Right, a_ij. So it''s essentially
  not what I</span> <span m=''2532000''>said. That''s the point.</span> </p><p><span
  m=''2533000''>If i does not equal j, you still have to think about a</span> <span
  m=''2537000''>single edge connecting i to j, right?</span> </p><p><span m=''2540000''>OK,
  so that''s a bit of a subtlety.</span> </p><p><span m=''2543000''>This is only intermediate
  vertices, so you could still go</span> <span m=''2547000''>from i to j via a single
  edge. That will cost a_ij.</span> </p><p><span m=''2552000''>If there is an edge:
  infinity.</span> </p><p><span m=''2554000''>If there isn''t one: that is a_ij.</span>
  </p><p><span m=''2557000''>So, OK, that gets us started. And then, we want a recurrence.</span>
  </p><p><span m=''2562000''>And, the recurrence is, well, maybe you get away with</span>
  <span m=''2566000''>all the vertices that you had before.</span> </p><p><span m=''2569000''>So,
  if you want to know paths that you had before,</span> <span m=''2572000''>so if
  you want to know paths that use one up to k,</span> <span m=''2576000''>maybe I
  just use one up to k minus one.</span> </p><p><span m=''2581000''>You could try
  that. Or, you could try using k.</span> </p><p><span m=''2584000''>So, either you
  use k or you don''t.</span> </p><p><span m=''2587000''>If you don''t, it''s got
  to be this.</span> </p><p><span m=''2589000''>If you do, then you''ve got to go
  to k.</span> </p><p><span m=''2592000''>So why not go to k at the end? So, you go
  from I to k using</span> <span m=''2597000''>the previous vertices. Obviously, you
  don''t want to</span> <span m=''2601000''>repeat k in there. And then, you go from
  k to j</span> <span m=''2604000''>somehow using vertices that are not k.</span>
  </p><p><span m=''2609000''>This should be pretty intuitive.</span> </p><p><span
  m=''2611000''>Again, I can draw a picture. So, either you never go to k,</span>
  <span m=''2615000''>and that''s this wiggly line. You go from i to j using things</span>
  <span m=''2620000''>only one up to k minus one. In other words,</span> <span m=''2623000''>here
  we have to use one up to k.</span> </p><p><span m=''2625000''>So, this just means
  don''t use k.</span> </p><p><span m=''2628000''>So, that''s this thing. Or, you
  use k somewhere in the</span> <span m=''2632000''>middle there. OK, it''s got to
  be one of the</span> <span m=''2635000''>two. And in this case,</span> <span m=''2637000''>you
  go from i to k using only smaller vertices,</span> <span m=''2640000''>because you
  don''t want to repeat k.</span> </p><p><span m=''2645000''>And here, you go from
  k to j using only smaller labeled</span> <span m=''2650000''>vertices. So, every
  path is one of the</span> <span m=''2654000''>two. So, we take the shortest of</span>
  <span m=''2658000''>these two subproblems. That''s the answer.</span> </p><p><span
  m=''2662000''>So, now we have a min of two things.</span> </p><p><span m=''2666000''>It
  takes constant time to compute.</span> </p><p><span m=''2669000''>So, we get a cubic
  algorithm. So, let me write it down.</span> </p><p><span m=''2676000''>So, this
  is the Floyd-Warshall algorithm.</span> </p><p><span m=''2681000''>I''ll write the
  name again. You give it a matrix A.</span> </p><p><span m=''2686000''>That''s all
  it really needs to know.</span> </p><p><span m=''2690000''>It codes everything.
  You copy C to A.</span> </p><p><span m=''2694000''>That''s the warm up. Right at
  time zero,</span> <span m=''2698000''>C equals A. And then you just have these</span>
  <span m=''2703000''>three loops for every value of k, for every value of i,</span>
  <span m=''2707000''>and for every value of j. You compute that min.</span> </p><p><span
  m=''2710000''>And if you think about it a little bit, that min is a</span> <span
  m=''2715000''>relaxation. Surprise, surprise.</span> </p><p><span m=''2747000''>So,
  that is the Floyd-Warshall algorithm.</span> </p><p><span m=''2751000''>And, the
  running time is clearly n^3, three nested loops,</span> <span m=''2758000''>constant
  time inside. So, we''re finally getting</span> <span m=''2762000''>something that
  is never worse than Bellman-Ford.</span> </p><p><span m=''2765000''>In the sparse
  case, it''s the same.</span> </p><p><span m=''2766000''>And anything denser, the
  number of edges is super</span> <span m=''2769000''>linear. This is strictly better
  than</span> <span m=''2771000''>Bellman-Ford. And, it''s better than</span> <span
  m=''2773000''>everything we''ve seen so far for all pair, shortest paths.</span>
  </p><p><span m=''2776000''>And, this handles negative weights; very simple algorithm,</span>
  <span m=''2779000''>even simpler than the one before.</span> </p><p><span m=''2781000''>It''s
  just relaxation within three loops.</span> </p><p><span m=''2783000''>What more
  could you ask for? And we need to check that this</span> <span m=''2787000''>is
  indeed what min we''re computing here,</span> <span m=''2789000''>except that the
  superscripts are omitted.</span> </p><p><span m=''2793000''>That''s, again, a bit
  of hand waving a bit.</span> </p><p><span m=''2795000''>It''s OK to omit subscripts
  because that can only mean that</span> <span m=''2799000''>you''re doing more relaxation
  techniques should be.</span> </p><p><span m=''2802000''>Doing more relaxations can
  never hurt you.</span> </p><p><span m=''2805000''>In particular, we do all the ones
  that we have</span> <span m=''2808000''>to. Therefore, we find the shortest</span>
  <span m=''2810000''>path weights. And, again, here,</span> <span m=''2812000''>we''re
  assuming that there is no negative weight cycles.</span> </p><p><span m=''2815000''>It
  shouldn''t be hard to find them, but you have to think</span> <span m=''2819000''>about
  that a little bit. OK, you could run another round</span> <span m=''2824000''>of
  Bellman-Ford, see if it relaxes in a new</span> <span m=''2827000''>edges again.
  For example,</span> <span m=''2829000''>I think there''s no nifty trick for that
  version.</span> </p><p><span m=''2833000''>And, we''re going to cover, that''s our
  second algorithm for</span> <span m=''2837000''>all pairs shortest paths. Before
  we go up to the third</span> <span m=''2841000''>algorithm, which is going to be
  the cleverest of them all,</span> <span m=''2846000''>the one Ring to rule them
  all, to switch trilogies,</span> <span m=''2850000''>we''re going to take a little
  bit of a diversion,</span> <span m=''2853000''>side story, whatever, and talk about
  transitive</span> <span m=''2857000''>closure briefly. This is just a good thing
  to</span> <span m=''2862000''>know about. And, it relates to the</span> <span m=''2865000''>algorithms
  we''ve seen so far. So, here''s a transitive closure</span> <span m=''2871000''>problem.
  I give you a directed graph,</span> <span m=''2874000''>and for all pair vertices,
  i and j, I want to compute this</span> <span m=''2879000''>number. It''s one if
  there''s a path from</span> <span m=''2883000''>i to j. From i to j,</span> <span
  m=''2886000''>OK, and then zero otherwise. OK, this is sort of like a</span> <span
  m=''2894000''>boring adjacency matrix with no weights, except it''s about paths</span>
  <span m=''2902000''>instead of being about edges. OK, so how can I compute this?</span>
  </p><p><span m=''2912000''>That''s very simple. How should I compute this?</span>
  </p><p><span m=''2919000''>This gives me a graph in some sense.</span> </p><p><span
  m=''2925000''>This is adjacency matrix of a new graph called the transitive</span>
  <span m=''2934000''>closure of my input graph. So, breadth first search,</span>
  <span m=''2941000''>yeah, good. So, all I need to do is find</span> <span m=''2945000''>shortest
  paths, and if the weights come out</span> <span m=''2948000''>infinity, then there''s
  no path. If it''s less than infinity,</span> <span m=''2952000''>that there''s a
  path. And so here,</span> <span m=''2955000''>so you are saying maybe I don''t care
  about the weights,</span> <span m=''2959000''>so I can run breadth first search
  n times,</span> <span m=''2962000''>and that will work indeed. So, if we do B times
  B of S,</span> <span m=''2967000''>so it''s maybe weird that I''m covering here
  in the middle,</span> <span m=''2971000''>but it''s just an interlude. So, we have,</span>
  <span m=''2976000''>then, something like V times E. OK, you can run any of these</span>
  <span m=''2982000''>algorithms. You could take Floyd-Warshall</span> <span m=''2986000''>for
  example. Why not?</span> </p><p><span m=''2988000''>OK, then it would just be V^
  I mean, you could run in any of</span> <span m=''2994000''>these algorithms with
  weights of one or zero, and just check</span> <span m=''3000000''>whether the values
  are infinity or not.</span> </p><p><span m=''3006000''>So, I mean, t_ij equals zero,
  if and only if the shortest</span> <span m=''3010000''>path weight from i to j is
  infinity.</span> </p><p><span m=''3012000''>So, just solve this. This is an easier
  problem than</span> <span m=''3016000''>shortest paths. It is, in fact,</span> <span
  m=''3018000''>strictly easier in a certain sense, because what''s going on</span>
  <span m=''3022000''>with transitive closure, and I just want to mention this</span>
  <span m=''3026000''>out of interest because transitive closure is a useful</span>
  <span m=''3030000''>thing to know about. Essentially,</span> <span m=''3033000''>what
  we are doing, let me get this right,</span> <span m=''3036000''>is using a different
  set of operators.</span> </p><p><span m=''3039000''>We''re using or and and, a logical
  or and and instead of</span> <span m=''3043000''>min and plus, OK, because we want
  to know,</span> <span m=''3046000''>if you think about a relaxation, in some sense,</span>
  <span m=''3049000''>maybe I should think about it in terms of this min.</span> </p><p><span
  m=''3053000''>So, if I want to know, is there a pathway from I to j</span> <span
  m=''3056000''>that uses vertices labeled one through k in the middle?</span> </p><p><span
  m=''3062000''>Well, either there is a path that doesn''t use the vertex k,</span>
  <span m=''3065000''>or there is a path that uses k, and then it would have to look</span>
  <span m=''3069000''>like that. OK, so there would have to be a</span> <span m=''3072000''>path
  here, and there would have to be a path there.</span> </p><p><span m=''3075000''>So,
  the min and plus get replaced with or and and.</span> </p><p><span m=''3078000''>And
  if you remember, this used to be plus,</span> <span m=''3081000''>and this used
  to be product in the matrix world.</span> </p><p><span m=''3084000''>So, plus is
  now like or. And, multiply is now like and,</span> <span m=''3088000''>which sounds
  very good, right?</span> </p><p><span m=''3091000''>Plus does feel like or, and
  multiply does feel like and</span> <span m=''3095000''>if you live in a zero-one
  world. So, in fact,</span> <span m=''3100000''>this is not quite the field Z mod
  two, but this is a good,</span> <span m=''3105000''>nice, field to work in. This
  is the Boolean world.</span> </p><p><span m=''3109000''>So, I''ll just write Boole.
  Good old Boole knows all about</span> <span m=''3115000''>this. It''s like his master''s
  thesis,</span> <span m=''3118000''>I think, talking about Boolean algebra.</span>
  </p><p><span m=''3123000''>And, this actually means that you can use fast matrix</span>
  <span m=''3126000''>multiply. You can use Strassen''s</span> <span m=''3129000''>algorithm,
  and the fancier algorithms, and you can compute</span> <span m=''3133000''>the transitive
  closure in subcubic time.</span> </p><p><span m=''3136000''>So, this is sub cubic
  if the edges are sparse.</span> </p><p><span m=''3139000''>But, it''s cubic in the
  worst case if there are lots of edges.</span> </p><p><span m=''3144000''>This is
  cubic. You can actually do better</span> <span m=''3147000''>using Strassen. So,
  I''ll just say you can do</span> <span m=''3150000''>it. No details here.</span>
  </p><p><span m=''3153000''>I think it should be, so in fact, there is a theorem.</span>
  </p><p><span m=''3157000''>This is probably not in the textbook, but there''s a
  theorem</span> <span m=''3161000''>that says transitive closure is just as hard
  as matrix multiply.</span> </p><p><span m=''3165000''>OK, they are equivalent. Their
  running times are the</span> <span m=''3169000''>same. We don''t know how long it
  takes</span> <span m=''3172000''>to do a matrix multiply over a field.</span> </p><p><span
  m=''3175000''>It''s somewhere between n^2 and n^2.3.</span> </p><p><span m=''3177000''>But,
  whatever the answer is: same for transitive closure.</span> </p><p><span m=''3183000''>OK,
  there''s the interlude. And that''s where we actually</span> <span m=''3189000''>get
  to use Strassen and friends. Remember, Strassen was n to the</span> <span m=''3196000''>log
  base two of seven algorithm. Remember that,</span> <span m=''3202000''>especially
  on the final. Those are things you should</span> <span m=''3208000''>have at the
  tip of your tongue. OK, the last algorithm we''re</span> <span m=''3215000''>going
  to cover is really going to build on what we saw last</span> <span m=''3219000''>time:
  Johnson''s algorithm. And, I''ve lost some of the</span> <span m=''3223000''>running
  times here. But, when we had unweighted</span> <span m=''3226000''>graphs, we could
  do all pairs really fast, just as fast as a</span> <span m=''3230000''>single source
  Bellman-Ford. That''s kind of nifty.</span> </p><p><span m=''3234000''>We don''t
  know how to improve Bellman-Ford in the single</span> <span m=''3238000''>source
  case. So, we can''t really help to get</span> <span m=''3242000''>anything better
  than V times E. And, if you remember running V</span> <span m=''3247000''>times
  Dijkstra, V times Dijkstra was about the</span> <span m=''3251000''>same. So, just
  put this in the recall</span> <span m=''3254000''>bubble here: V times Dijkstra
  would give us V times E plus V^2</span> <span m=''3259000''>log V. And, if you ignore
  that log</span> <span m=''3261000''>factor, this is just VE. OK, so this was really
  good.</span> </p><p><span m=''3265000''>Dijkstra was great. And this was for nonnegative</span>
  <span m=''3269000''>edge weights. So, with negative edge weights,</span> <span m=''3274000''>somehow
  we''d like to get the same running time.</span> </p><p><span m=''3278000''>Now,
  how might I get the same running time?</span> </p><p><span m=''3281000''>Well, it
  would be really nice if I could use Dijkstra.</span> </p><p><span m=''3285000''>Of
  course, Dijkstra doesn''t work with negative weights.</span> </p><p><span m=''3289000''>So
  what could I do? What would I hope to do?</span> </p><p><span m=''3293000''>What
  could I hope to? Suppose I want,</span> <span m=''3296000''>in the middle of the
  algorithm, it says run Dijkstra n times.</span> </p><p><span m=''3302000''>Then,
  what should I do to prepare for that?</span> </p><p><span m=''3305000''>Make all
  the weights positive, or nonnegative.</span> </p><p><span m=''3309000''>Why not,
  right? We''re being wishful thinking.</span> </p><p><span m=''3313000''>That''s
  what we''ll do. So, this is called graph</span> <span m=''3317000''>re-weighting.
  And, what''s cool is we actually</span> <span m=''3321000''>already know how to
  do it. We just don''t know that we know</span> <span m=''3326000''>how to do it.
  But I know that we know that we</span> <span m=''3330000''>know how to do it. You
  don''t yet know that we know</span> <span m=''3334000''>that I know that we know
  how to do it.</span> </p><p><span m=''3339000''>So, it turns out you can re-weight
  the vertices.</span> </p><p><span m=''3341000''>So, at the end of the last class
  someone asked me,</span> <span m=''3344000''>can you just, like, add the same weight
  to</span> <span m=''3346000''>all the edges? That doesn''t work.</span> </p><p><span
  m=''3348000''>Not quite, because different paths have different numbers of</span>
  <span m=''3351000''>edges. What we are going to do is add</span> <span m=''3353000''>a
  particular weight to each vertex.</span> </p><p><span m=''3355000''>What does that
  mean? Well, because we really only</span> <span m=''3358000''>have weights on the
  edges, here''s what well do.</span> </p><p><span m=''3362000''>We''ll re-weight
  each edge, so, (u,v), let''s say,</span> <span m=''3366000''>going to go back into
  graph speak instead of matrix speak,</span> <span m=''3372000''>(u,v) instead of
  I and j, and we''ll call this modified</span> <span m=''3377000''>weight w_h. h
  is our function.</span> </p><p><span m=''3380000''>It gives us a number for every
  vertex.</span> </p><p><span m=''3384000''>And, it''s just going to be the old weight
  of that edge plus the</span> <span m=''3390000''>weight of the start vertex minus
  the weight of the terminating</span> <span m=''3396000''>vertex. I''m sure these
  have good names.</span> </p><p><span m=''3400000''>One of these is the head, and
  the other is the tail,</span> <span m=''3403000''>but I can never remember which.
  OK, so we''ve directed edge</span> <span m=''3407000''>(u,v). Just add one of them;</span>
  <span m=''3408000''>subtract the other. And, it''s a directed edge,</span> <span
  m=''3411000''>so that''s a consistent definition.</span> </p><p><span m=''3413000''>OK,
  so that''s called re-weighting.</span> </p><p><span m=''3415000''>Now, this is actually
  a theorem.</span> </p><p><span m=''3418000''>If you do this, then, let''s say,</span>
  <span m=''3423000''>for any vertices, u and v in the graph,</span> <span m=''3430000''>for
  any two vertices, all paths from u to v have the</span> <span m=''3438000''>same
  weight as they did before, well, not quite.</span> </p><p><span m=''3447000''>They
  have the same re-weighting.</span> </p><p><span m=''3454000''>So, if you look at
  all the different paths and you say,</span> <span m=''3457000''>well, what''s the
  difference between vh, well,</span> <span m=''3459000''>sorry, let''s say delta,
  which is the old shortest</span> <span m=''3462000''>paths, and deltas of h, which
  is the shortest path</span> <span m=''3465000''>weights according to this new weight
  function,</span> <span m=''3468000''>then that difference is the same.</span> </p><p><span
  m=''3470000''>So, we''ll say that all these paths are re-weighted by the</span>
  <span m=''3473000''>same amounts. OK, this is actually a</span> <span m=''3475000''>statement
  about all paths, not just shortest paths.</span> </p><p><span m=''3480000''>There
  we go. OK, to how many people is this</span> <span m=''3485000''>obvious already?
  A few, yeah,</span> <span m=''3488000''>it is. And what''s the one word?</span>
  </p><p><span m=''3492000''>OK, it''s maybe not that obvious.</span> </p><p><span
  m=''3496000''>All right, shout out the word when you figure it out.</span> </p><p><span
  m=''3503000''>Meanwhile, I''ll write out this rather verbose proof.</span> </p><p><span
  m=''3509000''>There''s a one word proof, still waiting.</span> </p><p><span m=''3516000''>So,
  let''s just take one of these paths that starts at u and</span> <span m=''3521000''>ends
  at v. Take any path.</span> </p><p><span m=''3523000''>We''re just going to see
  what its new weight is relative to</span> <span m=''3529000''>its old weight. And
  so, let''s just write out</span> <span m=''3533000''>w_h of the path, which we define
  in the usual</span> <span m=''3537000''>way as the sum over all edges of the new
  weight of the edge from</span> <span m=''3543000''>v_i to v_i plus one. Do you have
  the word?</span> </p><p><span m=''3549000''>No? Tough puzzle then,</span> <span
  m=''3551000''>OK. So that''s the definition of the</span> <span m=''3555000''>weight
  of a path. And, then we know this thing is</span> <span m=''3560000''>just w of
  v_i, v_i plus one.</span> </p><p><span m=''3563000''>I''ll get it right, plus the
  weight of the first</span> <span m=''3567000''>vertex, plus, sorry, the re-weighting
  of v_i</span> <span m=''3572000''>minus the re-weighting of v_i plus one.</span>
  </p><p><span m=''3578000''>This is all in parentheses that''s summed over I.</span>
  </p><p><span m=''3582000''>Now I need the magic word. Telescopes, good.</span> </p><p><span
  m=''3586000''>Now this is obvious: each of these telescopes with</span> <span m=''3591000''>an
  extra previous, except the very beginning and</span> <span m=''3595000''>the very
  end. So, this is the sum of these</span> <span m=''3599000''>weights of edges, but
  then outside the sum,</span> <span m=''3603817''>we have plus h of v_1, and minus
  h of v_k.</span> </p><p><span m=''3609000''>OK, those guys don''t quite cancel.</span>
  </p><p><span m=''3611933''>We''re not looking at a cycle, just a path.</span> </p><p><span
  m=''3615577''>And, this thing is just w of the path, as this is the normal</span>
  <span m=''3620822''>weight of the path. And so the change,</span> <span m=''3624111''>the
  difference between w_h of P and w of P is this thing,</span> <span m=''3629088''>which
  is just h of u minus h of v.</span> </p><p><span m=''3633000''>And, the point is
  that''s the same as long as you fix the</span> <span m=''3636744''>endpoints, u
  and v, of the shortest path,</span> <span m=''3639468''>you''re changing this path
  weight by the same thing for all</span> <span m=''3643348''>paths. This is for any
  path from u to</span> <span m=''3645800''>v, and that proves the theorem. So, the
  one word here was</span> <span m=''3649612''>telescopes. These change in weights</span>
  <span m=''3651927''>telescope over any path. Therefore, if we want to find</span>
  <span m=''3655536''>shortest paths, you just find the shortest</span> <span m=''3658327''>paths
  in this re-weighted version, and then you just</span> <span m=''3661800''>change
  it by this one amount. You subtract off this amount</span> <span m=''3666848''>instead
  of adding it. That will give you the shortest</span> <span m=''3670281''>path weight
  in the original weights.</span> </p><p><span m=''3672591''>OK, so this is a tool.
  We now know how to change</span> <span m=''3675694''>weights in the graph. But what
  we really want is to</span> <span m=''3678995''>change weights in the graph so that
  the weights all come out</span> <span m=''3682889''>nonnegative. OK, how do we do
  that?</span> </p><p><span m=''3685134''>Why in the world would there be a function,
  h,</span> <span m=''3688105''>that makes all the edge weights nonnegative?</span>
  </p><p><span m=''3692000''>It doesn''t make sense. It turns out we already know.</span>
  </p><p><span m=''3702851''>So, I should write down this consequence.</span> </p><p><span
  m=''3732000''>Let me get this in the right order.</span> </p><p><span m=''3734193''>So
  in particular, the shortest path changes by</span> <span m=''3737096''>this amount.
  And if you want to know this</span> <span m=''3739677''>value, you just move the
  stuff to the other side.</span> </p><p><span m=''3742774''>So, we compute deltas
  of h, then we can compute delta.</span> </p><p><span m=''3746193''>That''s the consequence
  here. How many people here pronounce</span> <span m=''3749935''>this word corollary?
  OK, and how many people</span> <span m=''3753981''>pronounce it corollary? Yeah,
  we are alone.</span> </p><p><span m=''3757599''>Usually get at least one other student,
  and they''re usually</span> <span m=''3762596''>Canadian or British or something.</span>
  </p><p><span m=''3765353''>I think that the accent. So, I always avoid pronouncing</span>
  <span m=''3770006''>his word unless I really think, it''s corollary,</span> <span
  m=''3773969''>and get it right. I at least say Z not Zed.</span> </p><p><span m=''3777587''>OK,
  here we go. So, what we want to do is find</span> <span m=''3783428''>one of these
  functions. I mean, let''s just write down</span> <span m=''3789371''>what we could
  hope to have. We want to find a re-weighted</span> <span m=''3795771''>function,
  h, the signs of weight to each vertex such that w_h of</span> <span m=''3802971''>(u,v)
  is nonnegative. That would be great for all</span> <span m=''3808457''>edges, all
  (u,v) in E. OK, then we could run Dijkstra.</span> </p><p><span m=''3814735''>We
  could run Dijkstra, get the delta h''s,</span> <span m=''3818264''>and then just
  undo the re-weighting,</span> <span m=''3821352''>and get what we want. And, that
  is Johnson''s</span> <span m=''3825147''>algorithm. The claim is that this is</span>
  <span m=''3828235''>always possible. OK, why should it always be</span> <span m=''3832029''>possible?
  Well, let''s look at this</span> <span m=''3834941''>constraint. w_h of (u,v) is
  that.</span> </p><p><span m=''3837764''>So, it''s w of (u,v) plus h of u minus h
  of V should be</span> <span m=''3842441''>nonnegative. Let me rewrite this a little</span>
  <span m=''3849691''>bit. I''m going to put these guys</span> <span m=''3854886''>over
  here. That would be the right thing,</span> <span m=''3861589''>h of v minus h of
  u is less than or equal to w of (u,v).</span> </p><p><span m=''3870805''>Does that
  look familiar? Did I get it right?</span> </p><p><span m=''3879068''>It should be
  right. Anyone seen that inequality</span> <span m=''3886496''>before? Yeah, yes,
  correct answer.</span> </p><p><span m=''3891826''>OK, where? In a previous lecture?</span>
  </p><p><span m=''3896993''>In the previous lecture. What is this called if I</span>
  <span m=''3906000''>replace h with x? Charles knows.</span> </p><p><span m=''3911166''>Good,
  anyone else remember all the way back to episode two?</span> </p><p><span m=''3920833''>I
  know there was a weekend. What''s this operator called?</span> </p><p><span m=''3931000''>Not
  subtraction but, I think I heard it,</span> <span m=''3934058''>oh man. All right,
  I''ll tell you.</span> </p><p><span m=''3936568''>It''s a difference constraint,
  all right?</span> </p><p><span m=''3939627''>This is the difference operator.</span>
  </p><p><span m=''3942058''>OK, it''s our good friend difference constraints.</span>
  </p><p><span m=''3945745''>So, this is what we want to satisfy.</span> </p><p><span
  m=''3948490''>We have a system of difference constraints.</span> </p><p><span m=''3951784''>h
  of V minus h of u should be, we want to find these.</span> </p><p><span m=''3955862''>These
  are our unknowns. Subject to these constraints,</span> <span m=''3959941''>we are
  given the w''s. Now, we know in these</span> <span m=''3965845''>difference constraints
  are satisfiable.</span> </p><p><span m=''3970995''>Can someone tell me when these
  constraints are satisfiable?</span> </p><p><span m=''3978855''>We know exactly when
  for any set of difference constraints.</span> </p><p><span m=''3986714''>You''ve
  got to remember the math.</span> </p><p><span m=''3992000''>Terminology, I can understand.</span>
  </p><p><span m=''3997649''>It''s hard to remember words unless you''re a linguist,</span>
  <span m=''4007779''>perhaps. So, when is the system of</span> <span m=''4014207''>different
  constraints satisfiable?</span> </p><p><span m=''4022000''>All right, you should
  definitely, very good.</span> </p><p><span m=''4028341''>[LAUGHTER] Yes, very good.</span>
  </p><p><span m=''4032027''>Someone brought their lecture notes: when the constraint
  graph</span> <span m=''4041023''>has no negative weight cycles. Good, thank you.</span>
  </p><p><span m=''4047806''>Now, what is the constraint graph?</span> </p><p><span
  m=''4054000''>OK, this has a one letter answer more or less.</span> </p><p><span
  m=''4057726''>I''ll accept the one letter answer.</span> </p><p><span m=''4060458''>What?
  A?</span> </p><p><span m=''4061038''>A: close. G.</span> </p><p><span m=''4061949''>Yeah,
  I mean, same thing.</span> </p><p><span m=''4063936''>Yeah, so the constraint graph
  is essentially G.</span> </p><p><span m=''4067745''>Actually, it is G. The constraint
  graph is G,</span> <span m=''4071388''>good. And, we prove this by adding a</span>
  <span m=''4074286''>new source for text, and connecting that to</span> <span m=''4077764''>everyone.
  But that''s sort of beside the</span> <span m=''4081766''>point. That was in order
  to actually</span> <span m=''4083898''>satisfy them. But this is our</span> <span
  m=''4085604''>characterization. So, if we assume that there are</span> <span m=''4088527''>no
  negative weight cycles in our graph, which we''ve been doing</span> <span m=''4092243''>all
  the time, then we know that this thing is</span> <span m=''4094923''>satisfiable.
  Therefore, there is an</span> <span m=''4096993''>assignment of this h''s. There
  is a re-weighting that</span> <span m=''4100100''>makes all the weights nonnegative.</span>
  </p><p><span m=''4102111''>Then we can run Dijkstra. OK, we''re done.</span> </p><p><span
  m=''4104548''>Isn''t that cool? And how do we satisfy these</span> <span m=''4107167''>constraints?
  We know how to do that with one</span> <span m=''4109786''>run of Bellman-Ford,
  which costs order VE,</span> <span m=''4112283''>which is less than V times Dijkstra.</span>
  </p><p><span m=''4116000''>So, that''s it, write down the details</span> <span m=''4119750''>somewhere.</span>
  </p><p><span m=''4140000''>So, this is Johnson''s algorithm.</span> </p><p><span
  m=''4143902''>This is the fanciest of them all.</span> </p><p><span m=''4147931''>It
  will be our fastest, all pairs shortest path</span> <span m=''4153723''>algorithm.
  So, the claim is,</span> <span m=''4157122''>we can find a function, h, from V to
  R such that the</span> <span m=''4163542''>modified weight of every edge is nonnegative
  for every edge,</span> <span m=''4170970''>(u,v), in our graph. And, we do that
  using</span> <span m=''4177366''>Bellman-Ford to solve the difference constraints.</span>
  </p><p><span m=''4197000''>These are exactly the different constraints that we were
  born to</span> <span m=''4201075''>solve that we learned to solve last time.</span>
  </p><p><span m=''4203663''>The graphs here are corresponding exactly if you</span>
  <span m=''4206704''>look back at the definition. Or, Bellman-Ford will tell us</span>
  <span m=''4210391''>that there is a negative weight cycle.</span> </p><p><span m=''4212785''>OK,
  great, so it''s not that we really have to assume that there</span> <span m=''4216796''>is
  no negative weight cycle. We''ll get to know.</span> </p><p><span m=''4219772''>And
  if your fancy, you can actually figure out the</span> <span m=''4222942''>minus
  infinities from this. But, at this point,</span> <span m=''4225918''>I just want
  to think about the case where there is no negative</span> <span m=''4229865''>weight
  cycle. But if there is,</span> <span m=''4233696''>we can find out that it exists,
  and that just tell the user.</span> </p><p><span m=''4239954''>OK, then we''d stop.
  Otherwise, there is no negative</span> <span m=''4245257''>weight cycle. Therefore,
  there is an</span> <span m=''4248969''>assignment that gives is nonnegative edge
  weights.</span> </p><p><span m=''4254166''>So, we just use it. We use it to run
  Dijkstra.</span> </p><p><span m=''4260000''>So, step two is, oh, I should say the
  running</span> <span m=''4262744''>time of all this is V times E. So, we''re just
  running</span> <span m=''4265987''>Bellman-Ford on exactly the input graph.</span>
  </p><p><span m=''4268419''>Plus, we add a source, if you recall,</span> <span m=''4270665''>to
  solve a set of difference constraints.</span> </p><p><span m=''4273160''>You add
  a source vertex, S, connected to everyone at</span> <span m=''4276340''>weight zero,
  run Bellman-Ford from there because we don''t have</span> <span m=''4280145''>a
  source here. We just have a graph.</span> </p><p><span m=''4282327''>We want to
  know all pairs. So, this, you can use to find</span> <span m=''4285758''>whether
  there is a negative weight cycle anywhere.</span> </p><p><span m=''4290000''>Or,
  we get this magic assignment.</span> </p><p><span m=''4293428''>So now, w_h is nonnegative,
  so we can run Dijkstra on w_h.</span> </p><p><span m=''4299535''>We''ll say, using
  w_h, so you compute w_h.</span> </p><p><span m=''4303821''>That takes linear time.
  And, we run Dijkstra for each</span> <span m=''4309392''>possible source. I''ll
  write this out explicitly.</span> </p><p><span m=''4314428''>We''ve had this in
  our minds several times.</span> </p><p><span m=''4320000''>But, when we said n times
  Dijkstra over n times BFS,</span> <span m=''4325368''>here it is. We want to compute
  delta sub h</span> <span m=''4329684''>now, of (u,v) for all V, and we do this separately
  for</span> <span m=''4335263''>all u. And so, the running time here</span> <span
  m=''4338947''>is VE plus V^2 log V. This is just V times the</span> <span m=''4343684''>running
  time of Dijkstra, which is E plus V log V.</span> </p><p><span m=''4350000''>OK,
  it happens that this term is the same as this one,</span> <span m=''4355084''>which
  is nice, because that means step one</span> <span m=''4359017''>costs us nothing
  asymptotically. OK, and then,</span> <span m=''4363334''>last step is, well, now
  we know delta h.</span> </p><p><span m=''4367075''>We just need to compute delta.
  So, for each pair of vertices,</span> <span m=''4372831''>we''ll call it (u,v),
  we just compute what the</span> <span m=''4377052''>original weights would be, so
  what delta (u,v) is.</span> </p><p><span m=''4383000''>And we can do that using
  this corollary.</span> </p><p><span m=''4387471''>It''s just delta sub h of (u,v)
  minus h of u plus h of v.</span> </p><p><span m=''4393777''>I got the signs right.
  Yeah, so this takes V^2 time,</span> <span m=''4399624''>also dwarfed by the running
  time of Dijkstra.</span> </p><p><span m=''4404668''>So, the overall running time
  of Johnson''s algorithm is just the</span> <span m=''4411777''>running time of step
  two, running Dijkstra n times --</span> <span m=''4431000''>-- which is pretty cool.
  When it comes to single source</span> <span m=''4434951''>shortest paths, Bellman-Ford
  is the best thing</span> <span m=''4438243''>for general weights. Dijkstra is the
  best thing for</span> <span m=''4441990''>nonnegative weights. But for all pair
  shortest</span> <span m=''4444976''>paths, we can skirt the whole negative weight
  issue by using</span> <span m=''4448890''>this magic we saw from Bellman-Ford.</span>
  </p><p><span m=''4451213''>But now, running Dijkstra n times, which is still the
  best</span> <span m=''4454995''>thing we know how to do, pretty much,</span> <span
  m=''4457383''>for the all pairs nonnegative weights, now we can do it for</span>
  <span m=''4461232''>general weights too, which is a pretty nice</span> <span m=''4464018''>combination
  of all the techniques we''ve seen.</span> </p><p><span m=''4468000''>In the trilogy,
  and along the way,</span> <span m=''4470217''>we saw lots of dynamic programming,
  which is always</span> <span m=''4473577''>good practice. Any questions?</span>
  </p><p><span m=''4475459''>This is the last new content lecture before the quiz.</span>
  </p><p><span m=''4478954''>On Wednesday it will be quiz review, if I recall correctly.</span>
  </p><p><span m=''4482852''>And then it''s Thanksgiving, so there''s no recitation.</span>
  </p><p><span m=''4486347''>And then the quiz starts on Monday.</span> </p><p><span
  m=''4488632''>So, study up. See you then.</span> </p>'
type: course
uid: 2d2648ca3df8461c1d2d4ed58d52fd1e

---
None